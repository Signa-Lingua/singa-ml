{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # type: ignore\n",
    "import os\n",
    "import time\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "import mediapipe as mp  # type: ignore\n",
    "\n",
    "from matplotlib import pyplot as plt # type: ignore\n",
    "from mediapipe.tasks import python  # type: ignore\n",
    "from mediapipe.tasks.python import vision  # type: ignore\n",
    "from mediapipe.framework.formats import landmark_pb2 # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils # drawing utilities\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup mediapipe task vision\n",
    "\n",
    "[google documentation](https://ai.google.dev/edge/api/mediapipe/python/mp/tasks/vision) for task vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_base_options = python.BaseOptions(model_asset_path=\"./tasks/face_landmarker.task\")\n",
    "hand_base_options = python.BaseOptions(model_asset_path=\"./tasks/hand_landmarker.task\")\n",
    "pose_base_options = python.BaseOptions(model_asset_path=\"./tasks/pose_landmarker.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_options = vision.FaceLandmarkerOptions(\n",
    "    base_options=face_base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    num_faces=1,\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    ")\n",
    "\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    ")\n",
    "\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    ")\n",
    "\n",
    "\n",
    "face_detector = vision.FaceLandmarker.create_from_options(face_options)\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup keypoint extractor from task vision to be used as landmarker drawer for cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LandmarkList = landmark_pb2.NormalizedLandmarkList # alias\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark # alias\n",
    "\n",
    "\n",
    "def create_landmark_list(landmarks, num_keypoints):\n",
    "    \"\"\"Creates a LandmarkList from a list of landmarks or fills with empty values if no landmarks are provided.\n",
    "\n",
    "    Args:\n",
    "        landmarks: A list of landmark objects, each containing x, y, z coordinates.\n",
    "        num_keypoints: The number of keypoints to be included in the LandmarkList.\n",
    "\n",
    "    Returns:\n",
    "        A LandmarkList containing the converted landmarks or empty values if no landmarks are provided.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    empty_landmarks = [\n",
    "        NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(num_keypoints)\n",
    "    ] # generate empty landmarks with all coordinates set to 0.0\n",
    "\n",
    "    return LandmarkList(\n",
    "        landmark=(\n",
    "            # convert provided landmarks to NormalizedLandmark objects or use empty landmarks\n",
    "            [NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks]\n",
    "            if landmarks\n",
    "            else empty_landmarks\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_keypoints_for_drawing(face_results, pose_results, hand_results):\n",
    "    \"\"\"Converts face, pose, and hand landmarks to corresponding prototype lists for drawing.\n",
    "\n",
    "    Args:\n",
    "        face_results: Object containing face landmark detection results.\n",
    "        pose_results: Object containing pose landmark detection results.\n",
    "        hand_results: Object containing hand landmark detection results.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing three LandmarkList messages: face_landmarks, pose_landmarks, and hand_landmarks.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # convert face landmarks to LandmarkList, using empty values if no landmarks are present\n",
    "    face_landmarks_proto = create_landmark_list(\n",
    "        face_results.face_landmarks[0] if face_results.face_landmarks else None, 478 * 3\n",
    "    )\n",
    "\n",
    "    # convert pose landmarks to LandmarkList, using empty values if no landmarks are present\n",
    "    pose_landmarks_proto = create_landmark_list(\n",
    "        pose_results.pose_landmarks[0] if pose_results.pose_landmarks else None, 33 * 4\n",
    "    )\n",
    "\n",
    "    # convert hand landmarks to LandmarkList, using empty values if no landmarks are present\n",
    "    hand_landmarks_proto = [\n",
    "        create_landmark_list(hand_landmarks, 21 * 3)\n",
    "        for hand_landmarks in (\n",
    "            hand_results.hand_landmarks\n",
    "            if hand_results.hand_landmarks\n",
    "            else [None, None]  # two hands\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return face_landmarks_proto, pose_landmarks_proto, hand_landmarks_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detection_landmark(\n",
    "    image,\n",
    "    face_landmarks_proto=None,\n",
    "    pose_landmarks_proto=None,\n",
    "    hand_landmarks_proto=None,\n",
    "):\n",
    "    # draw landmark face\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        face_landmarks_proto,\n",
    "        mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "        drawer.DrawingSpec(color=(80, 60, 20), thickness=1, circle_radius=1),\n",
    "        drawer.DrawingSpec(color=(80, 146, 241), thickness=1, circle_radius=1),\n",
    "    )\n",
    "\n",
    "    # draw landmark pose\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks_proto,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    # draw landmark for both hand (right, left)\n",
    "    for idx in range(len(hand_landmarks_proto)):\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks_proto[idx],\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup keypoint extractor from task vision to be saved as dataset as npy (numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_for_dataset(face_results, pose_results, hand_results):\n",
    "    \"\"\"Extracts keypoints from face, pose, and hand results for dataset creation.\n",
    "\n",
    "    Handles cases with zero, one, or two hands, assigning hand keypoints based\n",
    "    on handedness information.\n",
    "\n",
    "    Args:\n",
    "      face_results: Object containing face landmark data (if available), assumed to\n",
    "                    have a `face_landmarks` attribute with landmark data.\n",
    "      pose_results: Object containing pose landmark data (if available), assumed to\n",
    "                    have a `pose_landmarks` attribute with landmark data.\n",
    "      hand_results: Object containing hand landmark data (if available), assumed to\n",
    "                    have `hand_landmarks` and `handedness` attributes.\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing three NumPy arrays representing flattened keypoints for face,\n",
    "      pose, and hand, respectively. Empty arrays are used for missing modalities.\n",
    "    \"\"\"\n",
    "\n",
    "    # extract face keypoints if available, otherwise return a zero-filled array\n",
    "    face_keypoints = (\n",
    "        np.array(\n",
    "            [\n",
    "                [landmark.x, landmark.y, landmark.z]\n",
    "                for landmark in face_results.face_landmarks[0]\n",
    "            ]\n",
    "        ).flatten()\n",
    "        if face_results.face_landmarks\n",
    "        else np.zeros(478 * 3)  # 478 landmarks with 3 coordinates each (x, y, z)\n",
    "    )\n",
    "\n",
    "    # extract pose keypoints if available, otherwise return a zero-filled array\n",
    "    pose_keypoints = (\n",
    "        np.array(\n",
    "            [\n",
    "                [landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "                for landmark in pose_results.pose_landmarks[0]\n",
    "            ]\n",
    "        ).flatten()\n",
    "        if pose_results.pose_landmarks\n",
    "        else np.zeros(33 * 4)  # 33 landmarks with 4 values each (x, y, z, visibility)\n",
    "    )\n",
    "\n",
    "    # initialize hand keypoints with zeros for two hands (right and left),\n",
    "    # each with 21 landmarks and 3 coordinates\n",
    "    hand_keypoints = np.zeros((2, 21, 3))\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate(\n",
    "            [face_keypoints, pose_keypoints, hand_keypoints.flatten()]\n",
    "        )\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for idx in range(len(hand_results.hand_landmarks)):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[idx][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_keypoints[handedness] = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in hand_results.hand_landmarks[idx]]\n",
    "        )\n",
    "\n",
    "    # flatten the hand keypoints array and concatenate it with face and pose keypoints\n",
    "    return np.concatenate([face_keypoints, pose_keypoints, hand_keypoints.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fps(start_time, frames):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return frames / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "\n",
    "def draw_fps(image, fps):\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        f\"FPS: {round(fps, 2)}\",\n",
    "        (10, 40),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1.5,\n",
    "        (0, 255, 0),\n",
    "        2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for saving the data (numpy array)\n",
    "DATA_PATH = os.path.join(\"../datasets\")\n",
    "\n",
    "# sign action to be detected\n",
    "ACTIONS = np.array(\n",
    "    [\n",
    "        \"hello\",\n",
    "        \"thanks\",\n",
    "        \"i-love-you\",\n",
    "        \"see-you-later\",\n",
    "        \"I\",\n",
    "        \"Father\",\n",
    "        \"Mother\",\n",
    "        \"Yes\",\n",
    "        \"No\",\n",
    "        \"Help\",\n",
    "        \"Please\",\n",
    "        \"Want\",\n",
    "        \"What\",\n",
    "        \"Again\",\n",
    "        \"Eat\",\n",
    "        \"Milk\",\n",
    "        \"More\",\n",
    "        \"Go To\",\n",
    "        \"Bathroom\",\n",
    "        \"Fine\",\n",
    "        \"Like\",\n",
    "        \"Learn\",\n",
    "        \"Sign\",\n",
    "        \"Done\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "ACTIONS = ACTIONS[3:6]\n",
    "\n",
    "# 60 videos worth of data (per label)\n",
    "videos_per_label = 60\n",
    "\n",
    "# 30 action per videos\n",
    "# NOTE: This does not affect how much the frame is\n",
    "action_per_video = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset folder\n",
    "\n",
    "try:\n",
    "  os.makedirs(os.path.join(DATA_PATH))\n",
    "except:\n",
    "    print(\"Dataset Folder Exists, skip creating new one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS # check what actions that will be collected for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: uncomment this code if you want to continue to add more videos per-action\n",
    "#       more data for video per-action the better the model will train\n",
    "\n",
    "# # create the folders to store the data for action per video (continous)\n",
    "# for action in ACTIONS:\n",
    "#     dirmax = np.max(np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int), initial=0) + 1\n",
    "\n",
    "#     for sequence in range(videos_per_label):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence + dirmax)))\n",
    "#             print(\"Folder created : \", action, \" - \", str(sequence + dirmax))\n",
    "#         except:\n",
    "#             print(\"skip\", action, \" - \", str(sequence + dirmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the folders to store the data for action per video\n",
    "for action in ACTIONS:\n",
    "    for sequence in range(videos_per_label):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "            print(\"Folder created : \", action, \" - \", str(sequence))\n",
    "        except:\n",
    "            print(\"skip\", action, \" - \", str(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_count = np.max(np.array(os.listdir(os.path.join(DATA_PATH, ACTIONS[0]))).astype(int)) + 1\n",
    "start_folder = folder_count - videos_per_label\n",
    "start_folder = 0\n",
    "\n",
    "print(start_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keypoint_as_np(action, sequence, action_length, keypoints):\n",
    "    np_path = os.path.join(DATA_PATH, action, str(sequence), str(action_length))\n",
    "\n",
    "    np.save(np_path, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPTURING THE DATA (OpenCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = lambda img: cv2.putText(\n",
    "    img,\n",
    "    \"STARTING COLLECTION\",\n",
    "    (120, 200),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    1,\n",
    "    (0, 255, 0),\n",
    "    4,\n",
    "    cv2.LINE_AA,\n",
    ")\n",
    "\n",
    "subh = lambda img, act, seq: cv2.putText(\n",
    "    img,\n",
    "    \"Collecting frames for {} Video Number {}\".format(act, seq),\n",
    "    (15, 12),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    0.5,\n",
    "    (0, 0, 255),\n",
    "    1,\n",
    "    cv2.LINE_AA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# set capture properties\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 480)  # set width to 480 pixels\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # set height to 480 pixels\n",
    "cap.set(cv2.CAP_PROP_FPS, 60)  # set frame rate to 60 FPS\n",
    "\n",
    "start_time = time.time()\n",
    "frames = 0\n",
    "isQuit = False\n",
    "\n",
    "summed = 0\n",
    "total = 3 * 60 * 30 # should be 5.400\n",
    "\n",
    "# NOTE (AHMAD): THE AVG FPS LIKE 12?? idk, maybe just device issue\n",
    "# TODO: Write better code prob\n",
    "while cap.isOpened():\n",
    "    for action in ACTIONS:\n",
    "        for sequence in range(start_folder, (start_folder + videos_per_label)):\n",
    "            for action_length in range(action_per_video):\n",
    "                success, image = cap.read()\n",
    "\n",
    "                if not success:\n",
    "                    print(\"Ignoring empty camera frame.\")\n",
    "                    continue\n",
    "\n",
    "                # NOTE: using flip image will screw'ed up some of the keypoints\n",
    "                #       data for training the model later\n",
    "                # image = cv2.flip(image, 1) # flip the image horizontally for a selfie-view display.\n",
    "\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # get current frame timestamp in milliseconds\n",
    "                timestamp_ms = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "\n",
    "                # convert cv image to mediapipe image format before being\n",
    "                # passed to face, pose and hand detector\n",
    "                annotated_image = mp.Image(\n",
    "                    image_format=mp.ImageFormat.SRGB, data=image_rgb\n",
    "                )\n",
    "\n",
    "                face_results = face_detector.detect_for_video(\n",
    "                    image=annotated_image, timestamp_ms=timestamp_ms\n",
    "                )\n",
    "\n",
    "                hand_results = hand_detector.detect_for_video(\n",
    "                    image=annotated_image, timestamp_ms=timestamp_ms + 1\n",
    "                )\n",
    "\n",
    "                pose_results = pose_detector.detect_for_video(\n",
    "                    image=annotated_image, timestamp_ms=timestamp_ms + 2\n",
    "                )\n",
    "\n",
    "                frames += 1\n",
    "                fps = calculate_fps(start_time, frames)\n",
    "\n",
    "                draw_fps(image_rgb, fps)\n",
    "\n",
    "                face_proto, pose_proto, hand_proto = extract_keypoints_for_drawing(\n",
    "                    face_results, pose_results, hand_results\n",
    "                )\n",
    "\n",
    "                draw_detection_landmark(\n",
    "                    image_rgb,\n",
    "                    face_landmarks_proto=face_proto,\n",
    "                    pose_landmarks_proto=pose_proto,\n",
    "                    hand_landmarks_proto=hand_proto,\n",
    "                )\n",
    "\n",
    "                if action_length == 0:\n",
    "                    head(image_rgb)\n",
    "                    subh(image_rgb, action, sequence)\n",
    "\n",
    "                    cv2.imshow(\n",
    "                        \"MediaPipe Detection\",\n",
    "                        cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB),\n",
    "                    )\n",
    "                    cv2.waitKey(1250)\n",
    "\n",
    "                else:\n",
    "                    subh(image_rgb, action, sequence)\n",
    "\n",
    "                    cv2.imshow(\n",
    "                        \"MediaPipe Detection\",\n",
    "                        cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB),\n",
    "                    )\n",
    "\n",
    "                keypoints = extract_keypoints_for_dataset(\n",
    "                    face_results, pose_results, hand_results\n",
    "                )\n",
    "\n",
    "                save_keypoint_as_np(action, sequence, action_length, keypoints)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                    isQuit = True\n",
    "                    break\n",
    "\n",
    "            if isQuit:\n",
    "                break\n",
    "\n",
    "        if isQuit:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
