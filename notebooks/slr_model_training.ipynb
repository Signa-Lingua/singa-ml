{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np # type: ignore\n",
    "import tensorflow as tf # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for saving the data (numpy array)\n",
    "DATA_PATH = os.path.join(\"../datasets\")\n",
    "\n",
    "# sign action to be detected\n",
    "ACTIONS = np.array(\n",
    "    [\n",
    "        \"hello\",\n",
    "        \"thanks\",\n",
    "        \"i-love-you\",\n",
    "        \"see-you-later\",\n",
    "        \"I\",\n",
    "        \"Father\",\n",
    "        \"Mother\",\n",
    "        \"Yes\",\n",
    "        \"No\",\n",
    "        \"Help\",\n",
    "        \"Please\",\n",
    "        \"Want\",\n",
    "        \"What\",\n",
    "        \"Again\",\n",
    "        \"Eat\",\n",
    "        \"Milk\",\n",
    "        \"More\",\n",
    "        \"Go To\",\n",
    "        \"Bathroom\",\n",
    "        \"Fine\",\n",
    "        \"Like\",\n",
    "        \"Learn\",\n",
    "        \"Sign\",\n",
    "        \"Done\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# NOTE: use the first 6 since we only have 6 label only for now\n",
    "ACTIONS = ACTIONS[:6]\n",
    "\n",
    "# x videos worth of data (per label)\n",
    "videos_per_label = np.max(np.array(os.listdir(os.path.join(DATA_PATH, ACTIONS[0]))).astype(int))\n",
    "\n",
    "# 30 action per videos\n",
    "# NOTE: This does not affect how much the frame is\n",
    "action_per_video = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output example:\n",
    "# {'hello': 0, 'thanks': 1, 'i-love-you': 2}\n",
    "labels_map = {label: index for index, label in enumerate(ACTIONS)}\n",
    "\n",
    "\n",
    "sequences, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map, videos_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterates over each action in the ACTIONS list.\n",
    "\n",
    "For each action, it will process multiple sequences of frames.\n",
    "\"\"\"\n",
    "for action in ACTIONS:\n",
    "\n",
    "    \"\"\"Iterates over each sequence for the current action\"\"\"\n",
    "    for sequence in range(videos_per_label):\n",
    "        # empty list (window) to hold the frames of the current sequence.\n",
    "        sequence_actions = []\n",
    "\n",
    "        \"\"\"\n",
    "        Frame Processing\n",
    "\n",
    "        Iterates over each frame in the current sequence, then constructs the file path to the numpy array for the current frame.\n",
    "        Prints the path to verify correctness, then loads the frame data from the numpy file.\n",
    "        \"\"\"\n",
    "        for frame_num in range(action_per_video):\n",
    "            # construct the path to the numpy file for the current frame\n",
    "            npy_path = os.path.join(\n",
    "                DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)\n",
    "            )\n",
    "\n",
    "            # load the frame data from the numpy file\n",
    "            result = np.load(npy_path)\n",
    "\n",
    "            # append the frame data to the current sequence (window)\n",
    "            sequence_actions.append(result)\n",
    "\n",
    "        # append the completed sequence to the sequences list\n",
    "        sequences.append(sequence_actions)\n",
    "\n",
    "        # append the corresponding label to the labels list\n",
    "        labels.append(labels_map[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences and labels lists into NumPy arrays that are suitable for use as input (X) and output (y) in machine learning models, particularly for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "\n",
    "# convert labels list to a one-hot encoded NumPy array\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the dataset into training and testing sets\n",
    "# specifies that 10% of the data should be used as the test set, and the remaining 90% should be used as the training set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the datasets depend on the total number of sequences and the sequence length. Assuming the code processes 30 sequences for each of the 3 actions, we have:\n",
    "\n",
    "    Total sequences = 30 sequences/action Ã— 3 actions = 90 sequences\n",
    "\n",
    "Given a test_size of 0.1, 10% of the data (approximately 9 sequences) will be in the test set, and 90% (approximately 81 sequences) will be in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model, load_model  # type: ignore\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Reshape, Bidirectional  # type: ignore\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau  # type: ignore\n",
    "from tensorflow.keras.regularizers import l2  # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input shape (30, 1692) where 30 is the sequence length and 1692 is the number of features per frame\n",
    "input_shape = (30, 1692)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic regarding TimeDistributed\n",
    "\n",
    "- https://stackoverflow.com/a/76796778/14182545\n",
    "- https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00\n",
    "\n",
    "    Another kind of layer that is the famous \"LSTM\" (or GRU). It is not mandatory but it will finalize the chronological resolution of inputs.\n",
    "\n",
    "    Dense without TimeDistributed computes perBatch.\n",
    "    TimeDistributed with Dense computes per Timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture CNN-LSTM_1L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asl-action-cnn-lstm_1l-560k\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# data normalization\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "# first Conv1D layer with L2 regularization\n",
    "model.add(\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    ")  # changed kernel size and filters\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# second Conv1D layer with L2 regularization\n",
    "model.add(\n",
    "    Conv1D(filters=128, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    ")  # changed kernel size and filters\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# third Conv1D layer with L2 regularization\n",
    "model.add(\n",
    "    Conv1D(filters=256, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    ")  # changed kernel size and filters\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# dense layer for feature extraction with L2 regularization\n",
    "model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# bidirectional LSTM layer with L2 regularization\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        LSTM(64, return_sequences=False, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    "    )\n",
    ")\n",
    "\n",
    "# dense layers for classification with dropout for regularization\n",
    "model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # slightly higher dropout rate, so it's not overfitting\n",
    "model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # slightly higher dropout rate, so it's not overfitting\n",
    "\n",
    "model.add(Dense(ACTIONS.shape[0], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_dir(base_dir, use_time=False):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # check existing log directories\n",
    "    existing_logs = [\n",
    "        d\n",
    "        for d in os.listdir(base_dir)\n",
    "        if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"train-\")\n",
    "    ]\n",
    "\n",
    "    # determine the new log directory name\n",
    "    if existing_logs and not use_time:\n",
    "        latest_log = max(existing_logs)\n",
    "        log_num = int(latest_log.split(\"-\")[1]) + 1\n",
    "        new_log_dir = os.path.join(base_dir, f\"train-{str(log_num).zfill(3)}\")\n",
    "\n",
    "    if not existing_logs and not use_time:\n",
    "        new_log_dir = os.path.join(base_dir, f\"train-001\")\n",
    "\n",
    "    if use_time:\n",
    "        new_log_dir = os.path.join(base_dir, f\"train-{current_time}\")\n",
    "\n",
    "    # create the new log directory\n",
    "    os.makedirs(new_log_dir)\n",
    "    print(f\"Created new log directory: {new_log_dir}\")\n",
    "\n",
    "    return new_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback  # type: ignore\n",
    "\n",
    "\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor=\"val_loss\", value=0.001, verbose=0, patience=20):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            raise ValueError(f\"Early stopping requires {self.monitor} available!\")\n",
    "\n",
    "        if current <= self.value:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch}: early stopping threshold reached with {self.monitor} = {current}\"\n",
    "                    )\n",
    "                self.model.stop_training = True\n",
    "        else:\n",
    "            self.wait = 0  # reset wait if the condition is not met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the optimizer with an initial learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# compile the model with the optimizer\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "Define the EarlyStopping callback with adjusted patience\n",
    "\n",
    "monitor  : monitor the `val_loss` for training.\n",
    "patience : sets the number of epochs to wait for an improvement,\n",
    "           in the monitored metric before stopping the training.\n",
    "           `patience=10` means that if the validation loss does not improve for 10 consecutive epochs,\n",
    "           the training will be stopped.\n",
    "\"\"\"\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)\n",
    "\n",
    "early_stopping_by_loss_val = EarlyStoppingByLossVal(monitor='val_loss', value=0.0001, verbose=1, patience=10)\n",
    "\n",
    "\"\"\"\n",
    "Define the ReduceLROnPlateau callback with adjusted factor and patience\n",
    "\n",
    "monitor  : monitor the `val_loss` for training.\n",
    "factor   : which the learning rate will be reduced. A factor=0.5 means the\n",
    "           learning rate will be halved when the metric has stopped improving.\n",
    "patience : sets the number of epochs with no improvement after which the learning rate will be reduced.\n",
    "           `patience=10` means if the validation loss does not improve for 10 consecutive epochs,\n",
    "           the learning rate will be reduced.\n",
    "min_lr   : lower bound on the learning rate, learning rate will not be reduced below `0.00001`,\n",
    "           ensuring it doesn't become too small.\n",
    "\"\"\"\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard callback for logging\n",
    "log_dir = os.path.join(create_log_dir(os.path.join('../drive/logs/asl_action_6'), True))\n",
    "\n",
    "tensor_board_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# train the model with the callbacks\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=600,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[tensor_board_cb, early_stopping, early_stopping_by_loss_val, reduce_lr],\n",
    "    # callbacks=[tensor_board_cb, reduce_lr],\n",
    "    # callbacks=[tensor_board_cb, early_stopping],\n",
    "    # callbacks=[tensor_board_cb],\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_filename(directory, base_name, extension):\n",
    "    # list all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    # filter files that start with the base_name and end with the extension\n",
    "    relevant_files = [\n",
    "        f for f in files if f.startswith(base_name) and f.endswith(extension)\n",
    "    ]\n",
    "\n",
    "    if not relevant_files and base_name == \"asl-action-weight\":\n",
    "        # if no relevant files found, start with 001\n",
    "        return f\"{base_name}-001.{extension}\"\n",
    "\n",
    "    if not base_name == \"asl-action-weight\":\n",
    "        return f\"{base_name}.{extension}\"\n",
    "\n",
    "    # extract the numeric part and find the highest number\n",
    "    numbers = [int(f[len(base_name) + 1 : -len(extension) - 1]) for f in relevant_files]\n",
    "    next_number = max(numbers) + 1\n",
    "\n",
    "    # format the next number with leading zeros to maintain the same length\n",
    "    next_filename = f\"{base_name}-{next_number:03d}.{extension}\"\n",
    "    return next_filename\n",
    "\n",
    "\n",
    "def model_save(\n",
    "    model, directory=\"../models/legacy\", base_name=\"asl-action-weight\", extension=\"h5\"\n",
    "):\n",
    "    next_filename = get_next_filename(directory, base_name, extension)\n",
    "    model_path = os.path.join(directory, next_filename)\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "    print(f\"Model saved as {next_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save(model, directory=\"../drive/models/keras\", base_name=\"...\", extension=\"keras\")\n",
    "model_save(model, directory=\"../drive/models/legacy\", base_name=\"...\", extension=\"h5\")\n",
    "\n",
    "# asl-action-cnn-lstm_1l-560k\n",
    "# asl-action-cnn-no_lstm-540k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(ACTIONS))\n",
    "plt.xticks(tick_marks, ACTIONS, rotation=45)\n",
    "plt.yticks(tick_marks, ACTIONS)\n",
    "\n",
    "# add labels\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "\n",
    "# compute and print accuracy score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
