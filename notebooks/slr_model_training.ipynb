{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np # type: ignore\n",
    "import tensorflow as tf # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for saving the data (numpy array)\n",
    "DATA_PATH = os.path.join(\"../datasets\")\n",
    "\n",
    "# sign action to be detected\n",
    "ACTIONS = np.array(\n",
    "    [\n",
    "        \"hello\",\n",
    "        \"thanks\",\n",
    "        \"i-love-you\",\n",
    "        \"see-you-later\",\n",
    "        \"I\",\n",
    "        \"Father\",\n",
    "        \"Mother\",\n",
    "        \"Yes\",\n",
    "        \"No\",\n",
    "        \"Help\",\n",
    "        \"Please\",\n",
    "        \"Want\",\n",
    "        \"What\",\n",
    "        \"Again\",\n",
    "        \"Eat\",\n",
    "        \"Milk\",\n",
    "        \"More\",\n",
    "        \"Go To\",\n",
    "        \"Bathroom\",\n",
    "        \"Fine\",\n",
    "        \"Like\",\n",
    "        \"Learn\",\n",
    "        \"Sign\",\n",
    "        \"Done\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# NOTE: use the first 6 since we only have 6 label only for now\n",
    "ACTIONS = ACTIONS[:6]\n",
    "\n",
    "# x videos worth of data (per label)\n",
    "videos_per_label = np.max(np.array(os.listdir(os.path.join(DATA_PATH, ACTIONS[0]))).astype(int)) + 1\n",
    "\n",
    "# 30 action per videos\n",
    "# NOTE: This does not affect how much the frame is\n",
    "action_per_video = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello', 'thanks', 'i-love-you', 'see-you-later', 'I', 'Father'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output example:\n",
    "# {'hello': 0, 'thanks': 1, 'i-love-you': 2}\n",
    "labels_map = {label: index for index, label in enumerate(ACTIONS)}\n",
    "\n",
    "sequences, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'hello': 0,\n",
       "  'thanks': 1,\n",
       "  'i-love-you': 2,\n",
       "  'see-you-later': 3,\n",
       "  'I': 4,\n",
       "  'Father': 5},\n",
       " 60)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_map, videos_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterates over each action in the ACTIONS list.\n",
    "\n",
    "For each action, it will process multiple sequences of frames.\n",
    "\"\"\"\n",
    "for action in ACTIONS:\n",
    "\n",
    "    \"\"\"Iterates over each sequence for the current action\"\"\"\n",
    "    for sequence in range(videos_per_label):\n",
    "        # empty list (window) to hold the frames of the current sequence.\n",
    "        sequence_actions = []\n",
    "\n",
    "        \"\"\"\n",
    "        Frame Processing\n",
    "\n",
    "        Iterates over each frame in the current sequence, then constructs the file path to the numpy array for the current frame.\n",
    "        Prints the path to verify correctness, then loads the frame data from the numpy file.\n",
    "        \"\"\"\n",
    "        for frame_num in range(action_per_video):\n",
    "            # construct the path to the numpy file for the current frame\n",
    "            npy_path = os.path.join(\n",
    "                DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)\n",
    "            )\n",
    "\n",
    "            # load the frame data from the numpy file\n",
    "            result = np.load(npy_path)\n",
    "\n",
    "            # append the frame data to the current sequence (window)\n",
    "            sequence_actions.append(result)\n",
    "\n",
    "        # append the completed sequence to the sequences list\n",
    "        sequences.append(sequence_actions)\n",
    "\n",
    "        # append the corresponding label to the labels list\n",
    "        labels.append(labels_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences and labels lists into NumPy arrays that are suitable for use as input (X) and output (y) in machine learning models, particularly for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "\n",
    "# convert labels list to a one-hot encoded NumPy array\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 360)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the dataset into training and testing sets\n",
    "# specifies that 10% of the data should be used as the test set, and the remaining 90% should be used as the training set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the datasets depend on the total number of sequences and the sequence length. Assuming the code processes 60 sequences for each of the 3 actions, we have:\n",
    "\n",
    "    Total sequences = 60 sequences/action Ã— 6 actions = 360 sequences\n",
    "\n",
    "Given a test_size of 0.2, 20% of the data (approximately 72 sequences) will be in the test set, and 80% (approximately 288 sequences) will be in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((288, 30, 1692), (72, 30, 1692), (288, 6), (72, 6))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, save_model  # type: ignore\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Dense, Dropout, LSTM, Bidirectional  # type: ignore\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau  # type: ignore\n",
    "from tensorflow.keras.regularizers import l2  # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input shape (30, 1692) where 30 is the sequence length and 1692 is the number of features per frame\n",
    "input_shape = (30, 1692)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic regarding TimeDistributed\n",
    "\n",
    "- https://stackoverflow.com/a/76796778/14182545\n",
    "- https://medium.com/smileinnovation/how-to-work-with-time-distributed-data-in-a-neural-network-b8b39aa4ce00\n",
    "\n",
    "    Another kind of layer that is the famous \"LSTM\" (or GRU). It is not mandatory but it will finalize the chronological resolution of inputs.\n",
    "\n",
    "    Dense without TimeDistributed computes perBatch.\n",
    "    TimeDistributed with Dense computes per Timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture CNN-LSTM_1L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asl-action-cnn-lstm_1l-560k\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# data normalization\n",
    "model.add(BatchNormalization(input_shape=input_shape))\n",
    "\n",
    "# first Conv1D layer with L2 regularization\n",
    "model.add(\n",
    "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    ")  # changed kernel size and filters\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# second Conv1D layer with L2 regularization\n",
    "model.add(\n",
    "    Conv1D(filters=128, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    ")  # changed kernel size and filters\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# third Conv1D layer with L2 regularization\n",
    "model.add(\n",
    "    Conv1D(filters=256, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    ")  # changed kernel size and filters\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# dense layer for feature extraction with L2 regularization\n",
    "model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# bidirectional LSTM layer with L2 regularization\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        LSTM(512, return_sequences=False, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
    "    )\n",
    ")\n",
    "\n",
    "# dense layers for classification with dropout for regularization\n",
    "model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # slightly higher dropout rate, so it's not overfitting\n",
    "model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))  # slightly higher dropout rate, so it's not overfitting\n",
    "\n",
    "model.add(Dense(ACTIONS.shape[0], activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (Batch  (None, 30, 1692)          6768      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 28, 64)            324928    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 14, 64)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 12, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 6, 128)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 4, 256)            98560     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 2, 256)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2, 64)             16448     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 64)             0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 1024)              2363392   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2974646 (11.35 MB)\n",
      "Trainable params: 2971262 (11.33 MB)\n",
      "Non-trainable params: 3384 (13.22 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_dir(base_dir, use_time=False):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # check existing log directories\n",
    "    existing_logs = [\n",
    "        d\n",
    "        for d in os.listdir(base_dir)\n",
    "        if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"train-\")\n",
    "    ]\n",
    "\n",
    "    # determine the new log directory name\n",
    "    if existing_logs and not use_time:\n",
    "        latest_log = max(existing_logs)\n",
    "        log_num = int(latest_log.split(\"-\")[1]) + 1\n",
    "        new_log_dir = os.path.join(base_dir, f\"train-{str(log_num).zfill(3)}\")\n",
    "\n",
    "    if not existing_logs and not use_time:\n",
    "        new_log_dir = os.path.join(base_dir, f\"train-001\")\n",
    "\n",
    "    if use_time:\n",
    "        new_log_dir = os.path.join(base_dir, f\"train-{current_time}\")\n",
    "\n",
    "    # create the new log directory\n",
    "    os.makedirs(new_log_dir)\n",
    "    print(f\"Created new log directory: {new_log_dir}\")\n",
    "\n",
    "    return new_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback  # type: ignore\n",
    "\n",
    "\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor=\"val_loss\", value=0.001, verbose=0, patience=20):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            raise ValueError(f\"Early stopping requires {self.monitor} available!\")\n",
    "\n",
    "        if current <= self.value:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch}: early stopping threshold reached with {self.monitor} = {current}\"\n",
    "                    )\n",
    "                self.model.stop_training = True\n",
    "        else:\n",
    "            self.wait = 0  # reset wait if the condition is not met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the optimizer with an initial learning rate\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# compile the model with the optimizer\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "Define the EarlyStopping callback with adjusted patience\n",
    "\n",
    "monitor  : monitor the `val_loss` for training.\n",
    "patience : sets the number of epochs to wait for an improvement,\n",
    "           in the monitored metric before stopping the training.\n",
    "           `patience=10` means that if the validation loss does not improve for 10 consecutive epochs,\n",
    "           the training will be stopped.\n",
    "\"\"\"\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "early_stopping_by_loss_val = EarlyStoppingByLossVal(monitor='val_loss', value=0.01, verbose=1, patience=10)\n",
    "\n",
    "\"\"\"\n",
    "Define the ReduceLROnPlateau callback with adjusted factor and patience\n",
    "\n",
    "monitor  : monitor the `val_loss` for training.\n",
    "factor   : which the learning rate will be reduced. A factor=0.5 means the\n",
    "           learning rate will be halved when the metric has stopped improving.\n",
    "patience : sets the number of epochs with no improvement after which the learning rate will be reduced.\n",
    "           `patience=10` means if the validation loss does not improve for 10 consecutive epochs,\n",
    "           the learning rate will be reduced.\n",
    "min_lr   : lower bound on the learning rate, learning rate will not be reduced below `0.00001`,\n",
    "           ensuring it doesn't become too small.\n",
    "\"\"\"\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new log directory: ../drive/logs/asl_action_6\\train-20240531-000128\n",
      "Epoch 1/1000\n",
      "9/9 [==============================] - 3s 84ms/step - loss: 0.9118 - accuracy: 0.6944 - val_loss: 0.8087 - val_accuracy: 0.6528 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.8244 - accuracy: 0.6076 - val_loss: 0.7937 - val_accuracy: 0.6528 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.8711 - accuracy: 0.5972 - val_loss: 0.8180 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.8089 - accuracy: 0.6528 - val_loss: 0.7519 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7503 - accuracy: 0.6528 - val_loss: 0.7358 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7063 - accuracy: 0.6840 - val_loss: 0.7352 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7195 - accuracy: 0.6910 - val_loss: 0.7151 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.6951 - accuracy: 0.6979 - val_loss: 0.7035 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6872 - accuracy: 0.6979 - val_loss: 0.6831 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6784 - accuracy: 0.6840 - val_loss: 0.6847 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.6748 - accuracy: 0.6632 - val_loss: 0.6743 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.6571 - accuracy: 0.6875 - val_loss: 0.6797 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6582 - accuracy: 0.6840 - val_loss: 0.6764 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6230 - accuracy: 0.7083 - val_loss: 0.6775 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.6586 - accuracy: 0.6667 - val_loss: 0.6691 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6554 - accuracy: 0.6979 - val_loss: 0.6831 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.7433 - accuracy: 0.6562 - val_loss: 0.6706 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6738 - accuracy: 0.6944 - val_loss: 0.6832 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6695 - accuracy: 0.7014 - val_loss: 0.6956 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6988 - accuracy: 0.6354 - val_loss: 0.7296 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.7185 - accuracy: 0.6736 - val_loss: 0.7098 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.7015 - accuracy: 0.7326 - val_loss: 0.6876 - val_accuracy: 0.6528 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6615 - accuracy: 0.7118 - val_loss: 0.6851 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6527 - accuracy: 0.7049 - val_loss: 0.6644 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6460 - accuracy: 0.7014 - val_loss: 0.6657 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6313 - accuracy: 0.7292 - val_loss: 0.6720 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 0.6292 - accuracy: 0.7049 - val_loss: 0.6695 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6222 - accuracy: 0.7153 - val_loss: 0.6562 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6273 - accuracy: 0.7431 - val_loss: 0.6554 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.6352 - accuracy: 0.7118 - val_loss: 0.6437 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6425 - accuracy: 0.7049 - val_loss: 0.7128 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.7237 - accuracy: 0.6736 - val_loss: 0.8666 - val_accuracy: 0.5139 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.7482 - accuracy: 0.6597 - val_loss: 0.6994 - val_accuracy: 0.6944 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6808 - accuracy: 0.7049 - val_loss: 0.7149 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6580 - accuracy: 0.7222 - val_loss: 0.6813 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6394 - accuracy: 0.7188 - val_loss: 0.6524 - val_accuracy: 0.6389 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.6325 - accuracy: 0.7222 - val_loss: 0.6264 - val_accuracy: 0.6944 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6029 - accuracy: 0.7604 - val_loss: 0.6404 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.8139 - accuracy: 0.7153 - val_loss: 0.7260 - val_accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.8164 - accuracy: 0.7083 - val_loss: 0.7765 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7243 - accuracy: 0.7500 - val_loss: 0.7775 - val_accuracy: 0.5556 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.7662 - accuracy: 0.7257 - val_loss: 0.7235 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6777 - accuracy: 0.7917 - val_loss: 0.6723 - val_accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6338 - accuracy: 0.7986 - val_loss: 0.6440 - val_accuracy: 0.7361 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6585 - accuracy: 0.7778 - val_loss: 0.6981 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.6823 - accuracy: 0.7500 - val_loss: 0.6421 - val_accuracy: 0.7222 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5822 - accuracy: 0.8333 - val_loss: 0.6486 - val_accuracy: 0.6806 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5925 - accuracy: 0.7535 - val_loss: 0.5916 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 49/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5535 - accuracy: 0.8438 - val_loss: 0.5619 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 50/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5367 - accuracy: 0.8194 - val_loss: 0.5739 - val_accuracy: 0.7361 - lr: 5.0000e-04\n",
      "Epoch 51/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5475 - accuracy: 0.8438 - val_loss: 0.5292 - val_accuracy: 0.7778 - lr: 5.0000e-04\n",
      "Epoch 52/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5255 - accuracy: 0.8438 - val_loss: 0.5226 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 53/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.5395 - accuracy: 0.8090 - val_loss: 0.5099 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 54/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5148 - accuracy: 0.8368 - val_loss: 0.5043 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 55/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5425 - accuracy: 0.8056 - val_loss: 0.5798 - val_accuracy: 0.7222 - lr: 5.0000e-04\n",
      "Epoch 56/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5308 - accuracy: 0.8229 - val_loss: 0.5301 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 57/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.5148 - accuracy: 0.8299 - val_loss: 0.5238 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 58/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4885 - accuracy: 0.8229 - val_loss: 0.4981 - val_accuracy: 0.8750 - lr: 5.0000e-04\n",
      "Epoch 59/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5170 - accuracy: 0.8438 - val_loss: 0.4893 - val_accuracy: 0.8056 - lr: 5.0000e-04\n",
      "Epoch 60/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4752 - accuracy: 0.8403 - val_loss: 0.4957 - val_accuracy: 0.8056 - lr: 5.0000e-04\n",
      "Epoch 61/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.5097 - accuracy: 0.8472 - val_loss: 0.5029 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 62/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.5053 - accuracy: 0.8403 - val_loss: 0.5077 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 63/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.5173 - accuracy: 0.8125 - val_loss: 0.4981 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 64/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5118 - accuracy: 0.8194 - val_loss: 0.4839 - val_accuracy: 0.7778 - lr: 5.0000e-04\n",
      "Epoch 65/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4747 - accuracy: 0.8438 - val_loss: 0.4936 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 66/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5306 - accuracy: 0.8299 - val_loss: 0.4907 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 67/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.4684 - accuracy: 0.8403 - val_loss: 0.4905 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 68/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4966 - accuracy: 0.8472 - val_loss: 0.5049 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 69/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4843 - accuracy: 0.8333 - val_loss: 0.4830 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 70/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.5605 - accuracy: 0.8021 - val_loss: 0.4857 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 71/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5016 - accuracy: 0.8229 - val_loss: 0.5804 - val_accuracy: 0.7222 - lr: 5.0000e-04\n",
      "Epoch 72/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4878 - accuracy: 0.8611 - val_loss: 0.5092 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 73/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4739 - accuracy: 0.8785 - val_loss: 0.5156 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 74/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4814 - accuracy: 0.8438 - val_loss: 0.4879 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 75/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4596 - accuracy: 0.8785 - val_loss: 0.4922 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 76/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4551 - accuracy: 0.8576 - val_loss: 0.4781 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 77/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4542 - accuracy: 0.8611 - val_loss: 0.4916 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 78/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4644 - accuracy: 0.8333 - val_loss: 0.4831 - val_accuracy: 0.7778 - lr: 5.0000e-04\n",
      "Epoch 79/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4448 - accuracy: 0.8542 - val_loss: 0.4707 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 80/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4526 - accuracy: 0.8056 - val_loss: 0.4686 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 81/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4390 - accuracy: 0.8333 - val_loss: 0.4665 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 82/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4316 - accuracy: 0.8681 - val_loss: 0.4701 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 83/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4480 - accuracy: 0.8507 - val_loss: 0.4711 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 84/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4318 - accuracy: 0.8507 - val_loss: 0.4760 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 85/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4283 - accuracy: 0.8507 - val_loss: 0.4722 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 86/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4340 - accuracy: 0.8611 - val_loss: 0.4619 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 87/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4263 - accuracy: 0.8438 - val_loss: 0.4580 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 88/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4361 - accuracy: 0.8472 - val_loss: 0.4653 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 89/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4160 - accuracy: 0.8715 - val_loss: 0.4665 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 90/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4097 - accuracy: 0.8646 - val_loss: 0.4600 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 91/1000\n",
      "9/9 [==============================] - 0s 36ms/step - loss: 0.4060 - accuracy: 0.8681 - val_loss: 0.4717 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 92/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4324 - accuracy: 0.8854 - val_loss: 0.4608 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 93/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4308 - accuracy: 0.8438 - val_loss: 0.4904 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 94/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4599 - accuracy: 0.8507 - val_loss: 0.4906 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 95/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4904 - accuracy: 0.8472 - val_loss: 0.4732 - val_accuracy: 0.7778 - lr: 5.0000e-04\n",
      "Epoch 96/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.5524 - accuracy: 0.8542 - val_loss: 0.5099 - val_accuracy: 0.7500 - lr: 5.0000e-04\n",
      "Epoch 97/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4787 - accuracy: 0.8368 - val_loss: 0.4856 - val_accuracy: 0.7639 - lr: 5.0000e-04\n",
      "Epoch 98/1000\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 0.4711 - accuracy: 0.8507 - val_loss: 0.4774 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 99/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4348 - accuracy: 0.8681 - val_loss: 0.4763 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 100/1000\n",
      "9/9 [==============================] - 0s 30ms/step - loss: 0.4543 - accuracy: 0.8438 - val_loss: 0.4787 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 101/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4428 - accuracy: 0.8507 - val_loss: 0.4741 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 102/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4388 - accuracy: 0.8542 - val_loss: 0.4715 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 103/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4288 - accuracy: 0.8819 - val_loss: 0.4669 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 104/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4139 - accuracy: 0.8646 - val_loss: 0.4605 - val_accuracy: 0.7917 - lr: 2.5000e-04\n",
      "Epoch 105/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4227 - accuracy: 0.8611 - val_loss: 0.4765 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 106/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4313 - accuracy: 0.8681 - val_loss: 0.4660 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 107/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.4036 - accuracy: 0.8785 - val_loss: 0.4512 - val_accuracy: 0.8333 - lr: 2.5000e-04\n",
      "Epoch 108/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4128 - accuracy: 0.8785 - val_loss: 0.4524 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 109/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4063 - accuracy: 0.8785 - val_loss: 0.4621 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 110/1000\n",
      "9/9 [==============================] - 0s 38ms/step - loss: 0.4321 - accuracy: 0.8681 - val_loss: 0.4578 - val_accuracy: 0.7917 - lr: 2.5000e-04\n",
      "Epoch 111/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4259 - accuracy: 0.8576 - val_loss: 0.4601 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 112/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4002 - accuracy: 0.8785 - val_loss: 0.4592 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 113/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.4283 - accuracy: 0.8646 - val_loss: 0.4470 - val_accuracy: 0.8333 - lr: 2.5000e-04\n",
      "Epoch 114/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4142 - accuracy: 0.8715 - val_loss: 0.4746 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 115/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4239 - accuracy: 0.8403 - val_loss: 0.4559 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 116/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4115 - accuracy: 0.8819 - val_loss: 0.4522 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 117/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3990 - accuracy: 0.8611 - val_loss: 0.4531 - val_accuracy: 0.8056 - lr: 2.5000e-04\n",
      "Epoch 118/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4000 - accuracy: 0.9028 - val_loss: 0.4495 - val_accuracy: 0.8056 - lr: 2.5000e-04\n",
      "Epoch 119/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3933 - accuracy: 0.8819 - val_loss: 0.4621 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 120/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4117 - accuracy: 0.8646 - val_loss: 0.4469 - val_accuracy: 0.7917 - lr: 2.5000e-04\n",
      "Epoch 121/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4103 - accuracy: 0.8889 - val_loss: 0.4584 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 122/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4151 - accuracy: 0.8889 - val_loss: 0.4621 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 123/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4049 - accuracy: 0.8576 - val_loss: 0.4526 - val_accuracy: 0.7917 - lr: 2.5000e-04\n",
      "Epoch 124/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4125 - accuracy: 0.9028 - val_loss: 0.4388 - val_accuracy: 0.8750 - lr: 2.5000e-04\n",
      "Epoch 125/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4037 - accuracy: 0.9028 - val_loss: 0.4490 - val_accuracy: 0.8472 - lr: 2.5000e-04\n",
      "Epoch 126/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4164 - accuracy: 0.8576 - val_loss: 0.4583 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 127/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3874 - accuracy: 0.8889 - val_loss: 0.4450 - val_accuracy: 0.8472 - lr: 2.5000e-04\n",
      "Epoch 128/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3971 - accuracy: 0.8889 - val_loss: 0.4369 - val_accuracy: 0.8889 - lr: 2.5000e-04\n",
      "Epoch 129/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3877 - accuracy: 0.9167 - val_loss: 0.4267 - val_accuracy: 0.8750 - lr: 2.5000e-04\n",
      "Epoch 130/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4124 - accuracy: 0.9028 - val_loss: 0.4439 - val_accuracy: 0.8611 - lr: 2.5000e-04\n",
      "Epoch 131/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3954 - accuracy: 0.8750 - val_loss: 0.4837 - val_accuracy: 0.7639 - lr: 2.5000e-04\n",
      "Epoch 132/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3924 - accuracy: 0.8819 - val_loss: 0.4221 - val_accuracy: 0.8750 - lr: 2.5000e-04\n",
      "Epoch 133/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4002 - accuracy: 0.9028 - val_loss: 0.4467 - val_accuracy: 0.8472 - lr: 2.5000e-04\n",
      "Epoch 134/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.4038 - accuracy: 0.8715 - val_loss: 0.4567 - val_accuracy: 0.8056 - lr: 2.5000e-04\n",
      "Epoch 135/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3748 - accuracy: 0.9167 - val_loss: 0.4272 - val_accuracy: 0.8750 - lr: 2.5000e-04\n",
      "Epoch 136/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3774 - accuracy: 0.8993 - val_loss: 0.4272 - val_accuracy: 0.8889 - lr: 2.5000e-04\n",
      "Epoch 137/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3993 - accuracy: 0.8924 - val_loss: 0.4304 - val_accuracy: 0.8750 - lr: 2.5000e-04\n",
      "Epoch 138/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3742 - accuracy: 0.9062 - val_loss: 0.4674 - val_accuracy: 0.7778 - lr: 2.5000e-04\n",
      "Epoch 139/1000\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.3824 - accuracy: 0.9062 - val_loss: 0.4255 - val_accuracy: 0.8611 - lr: 2.5000e-04\n",
      "Epoch 140/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3784 - accuracy: 0.9132 - val_loss: 0.4482 - val_accuracy: 0.8472 - lr: 2.5000e-04\n",
      "Epoch 141/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3972 - accuracy: 0.8715 - val_loss: 0.5104 - val_accuracy: 0.8194 - lr: 2.5000e-04\n",
      "Epoch 142/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4194 - accuracy: 0.8993 - val_loss: 0.5212 - val_accuracy: 0.7917 - lr: 2.5000e-04\n",
      "Epoch 143/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.4046 - accuracy: 0.8993 - val_loss: 0.4298 - val_accuracy: 0.8889 - lr: 1.2500e-04\n",
      "Epoch 144/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3625 - accuracy: 0.9271 - val_loss: 0.4545 - val_accuracy: 0.8333 - lr: 1.2500e-04\n",
      "Epoch 145/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3829 - accuracy: 0.9028 - val_loss: 0.4230 - val_accuracy: 0.8611 - lr: 1.2500e-04\n",
      "Epoch 146/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3488 - accuracy: 0.9306 - val_loss: 0.4028 - val_accuracy: 0.8750 - lr: 1.2500e-04\n",
      "Epoch 147/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3490 - accuracy: 0.9375 - val_loss: 0.4047 - val_accuracy: 0.8889 - lr: 1.2500e-04\n",
      "Epoch 148/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3642 - accuracy: 0.9306 - val_loss: 0.4302 - val_accuracy: 0.8889 - lr: 1.2500e-04\n",
      "Epoch 149/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3460 - accuracy: 0.9201 - val_loss: 0.4341 - val_accuracy: 0.8750 - lr: 1.2500e-04\n",
      "Epoch 150/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3576 - accuracy: 0.9306 - val_loss: 0.4086 - val_accuracy: 0.8889 - lr: 1.2500e-04\n",
      "Epoch 151/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3450 - accuracy: 0.9306 - val_loss: 0.4099 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 152/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3568 - accuracy: 0.9132 - val_loss: 0.4102 - val_accuracy: 0.8889 - lr: 1.2500e-04\n",
      "Epoch 153/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3278 - accuracy: 0.9444 - val_loss: 0.3991 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 154/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3600 - accuracy: 0.9340 - val_loss: 0.4051 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 155/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3668 - accuracy: 0.9444 - val_loss: 0.4322 - val_accuracy: 0.8889 - lr: 1.2500e-04\n",
      "Epoch 156/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3210 - accuracy: 0.9549 - val_loss: 0.4092 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 157/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3405 - accuracy: 0.9271 - val_loss: 0.3939 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 158/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3584 - accuracy: 0.9340 - val_loss: 0.3711 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 159/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3292 - accuracy: 0.9583 - val_loss: 0.3936 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 160/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3326 - accuracy: 0.9306 - val_loss: 0.4444 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 161/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.4161 - accuracy: 0.9097 - val_loss: 0.3647 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 162/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3706 - accuracy: 0.9271 - val_loss: 0.3748 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 163/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3348 - accuracy: 0.9549 - val_loss: 0.4060 - val_accuracy: 0.9306 - lr: 1.2500e-04\n",
      "Epoch 164/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3645 - accuracy: 0.8993 - val_loss: 0.3723 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 165/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3397 - accuracy: 0.9514 - val_loss: 0.3615 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 166/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3218 - accuracy: 0.9514 - val_loss: 0.3621 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 167/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3519 - accuracy: 0.9410 - val_loss: 0.3712 - val_accuracy: 0.9306 - lr: 1.2500e-04\n",
      "Epoch 168/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.3203 - accuracy: 0.9444 - val_loss: 0.4204 - val_accuracy: 0.9028 - lr: 1.2500e-04\n",
      "Epoch 169/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3498 - accuracy: 0.9375 - val_loss: 0.3531 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 170/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3555 - accuracy: 0.9583 - val_loss: 0.3574 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 171/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3426 - accuracy: 0.9306 - val_loss: 0.3822 - val_accuracy: 0.9306 - lr: 1.2500e-04\n",
      "Epoch 172/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3386 - accuracy: 0.9306 - val_loss: 0.3413 - val_accuracy: 0.9722 - lr: 1.2500e-04\n",
      "Epoch 173/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3367 - accuracy: 0.9479 - val_loss: 0.3404 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 174/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3366 - accuracy: 0.9375 - val_loss: 0.3707 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 175/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3406 - accuracy: 0.9514 - val_loss: 0.3525 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 176/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3111 - accuracy: 0.9653 - val_loss: 0.3511 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 177/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3049 - accuracy: 0.9722 - val_loss: 0.3954 - val_accuracy: 0.9167 - lr: 1.2500e-04\n",
      "Epoch 178/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3138 - accuracy: 0.9583 - val_loss: 0.3501 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 179/1000\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2964 - accuracy: 0.9618 - val_loss: 0.3338 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 180/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.3232 - accuracy: 0.9688 - val_loss: 0.3346 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 181/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2945 - accuracy: 0.9618 - val_loss: 0.3614 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 182/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3076 - accuracy: 0.9514 - val_loss: 0.3424 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 183/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3324 - accuracy: 0.9514 - val_loss: 0.3072 - val_accuracy: 0.9722 - lr: 1.2500e-04\n",
      "Epoch 184/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2953 - accuracy: 0.9618 - val_loss: 0.3142 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 185/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3348 - accuracy: 0.9583 - val_loss: 0.3424 - val_accuracy: 0.9444 - lr: 1.2500e-04\n",
      "Epoch 186/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3051 - accuracy: 0.9618 - val_loss: 0.2910 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 187/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3139 - accuracy: 0.9549 - val_loss: 0.2790 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 188/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2840 - accuracy: 0.9688 - val_loss: 0.3069 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 189/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.3008 - accuracy: 0.9549 - val_loss: 0.2938 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 190/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2721 - accuracy: 0.9826 - val_loss: 0.2851 - val_accuracy: 0.9722 - lr: 1.2500e-04\n",
      "Epoch 191/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2973 - accuracy: 0.9618 - val_loss: 0.3620 - val_accuracy: 0.9306 - lr: 1.2500e-04\n",
      "Epoch 192/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2685 - accuracy: 0.9896 - val_loss: 0.2752 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 193/1000\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.2884 - accuracy: 0.9722 - val_loss: 0.2773 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 194/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2724 - accuracy: 0.9826 - val_loss: 0.2791 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 195/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.3065 - accuracy: 0.9653 - val_loss: 0.2923 - val_accuracy: 0.9722 - lr: 1.2500e-04\n",
      "Epoch 196/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3023 - accuracy: 0.9653 - val_loss: 0.2526 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 197/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.3151 - accuracy: 0.9618 - val_loss: 0.3138 - val_accuracy: 0.9722 - lr: 1.2500e-04\n",
      "Epoch 198/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2566 - accuracy: 0.9861 - val_loss: 0.3284 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 199/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2731 - accuracy: 0.9792 - val_loss: 0.2531 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 200/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2789 - accuracy: 0.9861 - val_loss: 0.2728 - val_accuracy: 0.9722 - lr: 1.2500e-04\n",
      "Epoch 201/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2820 - accuracy: 0.9722 - val_loss: 0.3020 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 202/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2674 - accuracy: 0.9861 - val_loss: 0.3141 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 203/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2721 - accuracy: 0.9722 - val_loss: 0.2918 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 204/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2576 - accuracy: 0.9826 - val_loss: 0.2538 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 205/1000\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 0.3163 - accuracy: 0.9722 - val_loss: 0.2712 - val_accuracy: 0.9861 - lr: 1.2500e-04\n",
      "Epoch 206/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2621 - accuracy: 0.9861 - val_loss: 0.3030 - val_accuracy: 0.9583 - lr: 1.2500e-04\n",
      "Epoch 207/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2512 - accuracy: 0.9826 - val_loss: 0.2712 - val_accuracy: 0.9722 - lr: 6.2500e-05\n",
      "Epoch 208/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2994 - accuracy: 0.9792 - val_loss: 0.2781 - val_accuracy: 0.9722 - lr: 6.2500e-05\n",
      "Epoch 209/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2583 - accuracy: 0.9896 - val_loss: 0.2767 - val_accuracy: 0.9861 - lr: 6.2500e-05\n",
      "Epoch 210/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2630 - accuracy: 0.9896 - val_loss: 0.2691 - val_accuracy: 0.9861 - lr: 6.2500e-05\n",
      "Epoch 211/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2896 - accuracy: 0.9722 - val_loss: 0.2936 - val_accuracy: 0.9583 - lr: 6.2500e-05\n",
      "Epoch 212/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2710 - accuracy: 0.9861 - val_loss: 0.2789 - val_accuracy: 0.9583 - lr: 6.2500e-05\n",
      "Epoch 213/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2598 - accuracy: 0.9896 - val_loss: 0.2954 - val_accuracy: 0.9583 - lr: 6.2500e-05\n",
      "Epoch 214/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2845 - accuracy: 0.9757 - val_loss: 0.2903 - val_accuracy: 0.9861 - lr: 6.2500e-05\n",
      "Epoch 215/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2547 - accuracy: 0.9896 - val_loss: 0.2905 - val_accuracy: 0.9861 - lr: 6.2500e-05\n",
      "Epoch 216/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2408 - accuracy: 0.9931 - val_loss: 0.2817 - val_accuracy: 0.9861 - lr: 6.2500e-05\n",
      "Epoch 217/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2464 - accuracy: 0.9861 - val_loss: 0.2881 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 218/1000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2457 - accuracy: 0.9931 - val_loss: 0.2781 - val_accuracy: 0.9722 - lr: 3.1250e-05\n",
      "Epoch 219/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2599 - accuracy: 0.9931 - val_loss: 0.2500 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 220/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2626 - accuracy: 0.9757 - val_loss: 0.2420 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 221/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2500 - accuracy: 0.9861 - val_loss: 0.2641 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 222/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2751 - accuracy: 0.9792 - val_loss: 0.2660 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 223/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2536 - accuracy: 0.9757 - val_loss: 0.2710 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 224/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2528 - accuracy: 0.9931 - val_loss: 0.2793 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 225/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2513 - accuracy: 0.9931 - val_loss: 0.2737 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 226/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2751 - accuracy: 0.9826 - val_loss: 0.2688 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 227/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.3134 - accuracy: 0.9653 - val_loss: 0.2641 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 228/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2535 - accuracy: 0.9861 - val_loss: 0.2683 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 229/1000\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 0.2750 - accuracy: 0.9757 - val_loss: 0.2660 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 230/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2665 - accuracy: 0.9826 - val_loss: 0.2598 - val_accuracy: 0.9861 - lr: 3.1250e-05\n",
      "Epoch 231/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2553 - accuracy: 0.9896 - val_loss: 0.2614 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 232/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2489 - accuracy: 0.9896 - val_loss: 0.2647 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 233/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2426 - accuracy: 0.9896 - val_loss: 0.2595 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 234/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2350 - accuracy: 0.9931 - val_loss: 0.2615 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 235/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2402 - accuracy: 0.9896 - val_loss: 0.2666 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 236/1000\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.2710 - accuracy: 0.9792 - val_loss: 0.2703 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 237/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2514 - accuracy: 0.9896 - val_loss: 0.2676 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 238/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2360 - accuracy: 0.9965 - val_loss: 0.2635 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 239/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2631 - accuracy: 0.9826 - val_loss: 0.2517 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 240/1000\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2480 - accuracy: 0.9931 - val_loss: 0.2466 - val_accuracy: 0.9861 - lr: 1.5625e-05\n",
      "Epoch 241/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2366 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 242/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2685 - accuracy: 0.9826 - val_loss: 0.2482 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 243/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2463 - accuracy: 0.9896 - val_loss: 0.2525 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 244/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2542 - accuracy: 0.9861 - val_loss: 0.2570 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 245/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2457 - accuracy: 0.9826 - val_loss: 0.2590 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 246/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2560 - accuracy: 0.9896 - val_loss: 0.2580 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 247/1000\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 0.2329 - accuracy: 0.9965 - val_loss: 0.2561 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 248/1000\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 0.2422 - accuracy: 0.9931 - val_loss: 0.2560 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 249/1000\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 0.2341 - accuracy: 0.9896 - val_loss: 0.2589 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 250/1000\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 0.2434 - accuracy: 0.9896 - val_loss: 0.2635 - val_accuracy: 0.9861 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# TensorBoard callback for logging\n",
    "log_dir = os.path.join(create_log_dir(os.path.join(\"../drive/logs/asl_action_6\"), True))\n",
    "\n",
    "tensor_board_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# train the model with the callbacks\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[tensor_board_cb, early_stopping, early_stopping_by_loss_val, reduce_lr],\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_filename(directory, base_name, extension):\n",
    "    # list all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    # filter files that start with the base_name and end with the extension\n",
    "    relevant_files = [\n",
    "        f for f in files if f.startswith(base_name) and f.endswith(extension)\n",
    "    ]\n",
    "\n",
    "    if not relevant_files and base_name == \"asl-action-weight\":\n",
    "        # if no relevant files found, start with 001\n",
    "        return f\"{base_name}-001.{extension}\"\n",
    "\n",
    "    if not base_name == \"asl-action-weight\":\n",
    "        return f\"{base_name}.{extension}\"\n",
    "\n",
    "    # extract the numeric part and find the highest number\n",
    "    numbers = [int(f[len(base_name) + 1 : -len(extension) - 1]) for f in relevant_files]\n",
    "    next_number = max(numbers) + 1\n",
    "\n",
    "    # format the next number with leading zeros to maintain the same length\n",
    "    next_filename = f\"{base_name}-{next_number:03d}.{extension}\"\n",
    "    return next_filename\n",
    "\n",
    "\n",
    "def model_save(\n",
    "    model, directory=\"../models/legacy\", base_name=\"asl-action-weight\", extension=\"h5\"\n",
    "):\n",
    "    next_filename = get_next_filename(directory, base_name, extension)\n",
    "    model_path = os.path.join(directory, next_filename)\n",
    "\n",
    "    save_model(model, model_path)\n",
    "\n",
    "    print(f\"Model saved as {next_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as asl-action-cnn-lstm_1l-6a-es_p30__rlr_f05_p10_lr1e5-560k.keras\n",
      "Model saved as asl-action-cnn-lstm_1l-6a-es_p30__rlr_f05_p10_lr1e5-560k.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_9884\\3448883768.py:31: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  save_model(model, model_path)\n"
     ]
    }
   ],
   "source": [
    "model_save(model, directory=\"../drive/models/keras\", base_name=\"asl-action-cnn-lstm_1l-6a-es_p30__rlr_f05_p10_lr1e5-2.9M\", extension=\"keras\")\n",
    "model_save(model, directory=\"../drive/models/legacy\", base_name=\"asl-action-cnn-lstm_1l-6a-es_p30__rlr_f05_p10_lr1e5-2.9M\", extension=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027F5B167AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9861111111111112\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(ACTIONS))\n",
    "plt.xticks(tick_marks, ACTIONS, rotation=45)\n",
    "plt.yticks(tick_marks, ACTIONS)\n",
    "\n",
    "# add labels\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "\n",
    "# compute and print accuracy score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
