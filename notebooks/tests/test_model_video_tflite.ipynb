{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pprint\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(model_asset_path=\"../tasks/hand_landmarker.task\")\n",
    "pose_base_options = python.BaseOptions(model_asset_path=\"../tasks/pose_landmarker.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.6,\n",
    "    min_hand_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.6,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.6,\n",
    "    min_pose_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.6,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_hand_landmark = np.zeros((2, 21, 3))  # right hand and left hand\n",
    "empty_pose_landmark = np.zeros(33 * 3)\n",
    "\n",
    "def to_landmark_data(\n",
    "    hand_results: vision.HandLandmarkerResult, pose_results: vision.PoseLandmarkerResult\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract keypoints from pose and hand results for dataset creation.\n",
    "    \"\"\"\n",
    "    pose_landmark = empty_pose_landmark\n",
    "    hand_landmark = empty_hand_landmark\n",
    "\n",
    "    if pose_results.pose_world_landmarks:\n",
    "        pose_landmark = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in pose_results.pose_world_landmarks[0]]\n",
    "        ).flatten()\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hlm in enumerate(hand_results.hand_world_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmark[handedness] = np.array([[lm.x, lm.y, lm.z] for lm in hlm])\n",
    "\n",
    "    return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 4)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return pose_landmarks, hand_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action lables\n",
    "ACTIONS = [\n",
    "    \"_\", \"Hello\", \"Thanks\", \"i-love-you\", \"I\", \"Yes\", \"No\", \"Help\", \"Please\",\n",
    "    \"Want\", \"Eat\", \"More\", \"Bathroom\", \"Learn\", \"Sign\",\n",
    "]\n",
    "\n",
    "# limit to x actions for preprocessing\n",
    "# NOTE: change this number into the amount of the dataset labels (if changed)\n",
    "ACTIONS = np.array(ACTIONS[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 0,\n",
      "  'name': 'serving_default_keras_tensor:0',\n",
      "  'quantization': (0.0, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([], dtype=float32),\n",
      "                              'zero_points': array([], dtype=int32)},\n",
      "  'shape': array([  1,  60, 225]),\n",
      "  'shape_signature': array([ -1,  60, 225]),\n",
      "  'sparsity_parameters': {}}]\n",
      "\n",
      "Output details:\n",
      "[{'dtype': <class 'numpy.float32'>,\n",
      "  'index': 68,\n",
      "  'name': 'StatefulPartitionedCall_1:0',\n",
      "  'quantization': (0.0, 0),\n",
      "  'quantization_parameters': {'quantized_dimension': 0,\n",
      "                              'scales': array([], dtype=float32),\n",
      "                              'zero_points': array([], dtype=int32)},\n",
      "  'shape': array([1, 4]),\n",
      "  'shape_signature': array([-1,  4]),\n",
      "  'sparsity_parameters': {}}]\n",
      "==================================================\n",
      "Expected input shape: [  1  60 225]\n"
     ]
    }
   ],
   "source": [
    "class TFLiteModel:\n",
    "    def __init__(self, prefix=\"singa_slr_v_\"):\n",
    "        self.model_dir = \"../../storage/models/tflite\"\n",
    "        self.prefix = prefix\n",
    "        self.interpreter = None\n",
    "        self.input_details = None\n",
    "        self.output_details = None\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "\n",
    "    def load_model(self, use_latest=True, version=\"\"):\n",
    "        if not use_latest and not version:\n",
    "            model_path = os.path.join(self.model_dir, f\"{self.prefix}{version}\")\n",
    "        else:\n",
    "            model_files = os.listdir(self.model_dir)\n",
    "\n",
    "            # filter model files by filename prefix\n",
    "            versions = [file for file in model_files if file.startswith(self.prefix)]\n",
    "\n",
    "            # extract version numbers from filenames\n",
    "            versions = [file.split(\"_\")[-1] for file in versions]\n",
    "\n",
    "            # convert version numbers to tuples of integers for comparison\n",
    "            versions_int = [tuple(map(int, v.split(\".\")[0])) for v in versions]\n",
    "\n",
    "            # find the index of the latest version\n",
    "            latest_index = versions_int.index(max(versions_int))\n",
    "\n",
    "            # load the latest model\n",
    "            latest_model_path = model_files[latest_index]\n",
    "            model_path = os.path.join(self.model_dir, latest_model_path)\n",
    "\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "\n",
    "        self.input_shape = self.input_details[0][\"index\"]\n",
    "        self.output_shape = self.output_details[0][\"index\"]\n",
    "\n",
    "        self.print_model_details()\n",
    "\n",
    "    def print_model_details(self):\n",
    "        print(\"Input details:\")\n",
    "        pprint.pprint(self.input_details)\n",
    "        print()\n",
    "        print(\"Output details:\")\n",
    "        pprint.pprint(self.output_details)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        input_shape = self.input_details[0][\"shape\"]\n",
    "        print(\"Expected input shape:\", input_shape)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        self.interpreter.set_tensor(self.input_shape, input_data)\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        result = self.interpreter.get_tensor(self.output_shape)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "model = TFLiteModel()\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, image, threshold, skip_word):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert into mediapipe numpy type support uint8, uint16, or float32\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # Convert cv image to mediapipe image format before being passed to detectors\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "    try:\n",
    "        hand_results = hand_detector.detect(image=mp_image)\n",
    "        pose_results = pose_detector.detect(image=mp_image)\n",
    "\n",
    "        landmarks = to_landmark_data(hand_results, pose_results)\n",
    "    except:\n",
    "        print(f\"frame {frame} skipped\")\n",
    "        return frame, None, time.time() - start_time\n",
    "\n",
    "    return frame, landmarks, time.time() - start_time\n",
    "\n",
    "\n",
    "def predict_from_video(vid):\n",
    "    clip = VideoFileClip(vid)\n",
    "\n",
    "    avg_exec_time = []\n",
    "\n",
    "    predictions = []\n",
    "    sequences = []\n",
    "\n",
    "    sentence = []\n",
    "    threshold = 0.2\n",
    "    skip_word = \"_\"\n",
    "\n",
    "    results = []\n",
    "    batch_size = 60\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_frame = {\n",
    "            executor.submit(\n",
    "                process_frame,\n",
    "                frame,\n",
    "                image,\n",
    "                threshold,\n",
    "                skip_word,\n",
    "            ): frame\n",
    "            for frame, image in enumerate(clip.iter_frames(fps=clip.fps))\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_frame):\n",
    "            frame, landmarks, exec_time = future.result()\n",
    "            avg_exec_time.append(exec_time)\n",
    "\n",
    "            if landmarks is not None:\n",
    "                results.append((frame, landmarks))\n",
    "\n",
    "    # sort the results by frame number to ensure the order is correct\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    for _, landmarks in results:\n",
    "        sequences.append(landmarks)\n",
    "\n",
    "        if len(sequences) < batch_size:\n",
    "            continue\n",
    "\n",
    "        # collect a batch of sequences\n",
    "        batch_motion = np.stack(sequences[-batch_size:]).astype(np.float32)\n",
    "        sequences = sequences[\n",
    "            -(batch_size - 40) :\n",
    "        ]  # keep the last 20 sequences for overlap\n",
    "\n",
    "        # ensure correct input shape by adding an extra dimension for batch size\n",
    "        batch_motion = np.expand_dims(batch_motion, axis=0)\n",
    "\n",
    "        # predict the entire batch\n",
    "        batch_result = model.predict(batch_motion)\n",
    "\n",
    "        print(batch_result)\n",
    "\n",
    "        for result in batch_result:\n",
    "            # len of results is 480 (which is the total frame)?\n",
    "            predicted = np.argmax(result)\n",
    "\n",
    "            if (not result[predicted] > threshold) or not (\n",
    "                ACTIONS[predicted] != skip_word\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            if not predictions or predicted != predictions[-1]:\n",
    "                predictions.append(predicted)\n",
    "\n",
    "    print(\"===\")\n",
    "    print(predictions)\n",
    "    print(\"===\")\n",
    "\n",
    "    for motion in predictions:\n",
    "        sentence.append(ACTIONS[motion])\n",
    "\n",
    "    return (\n",
    "        sentence,\n",
    "        len(results),\n",
    "        {\n",
    "            \"avg_exec_time\": avg_exec_time,\n",
    "            \"total_exec_time\": sum(avg_exec_time),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7529997e-08 9.9999416e-01 2.5141139e-08 5.8510623e-06]]\n",
      "[[2.4833531e-08 9.9999154e-01 2.6527182e-08 8.5217362e-06]]\n",
      "[[1.2964776e-05 9.9575233e-01 4.0977676e-03 1.3703598e-04]]\n",
      "[[9.999311e-01 7.603475e-09 6.263397e-05 6.314619e-06]]\n",
      "[[2.0190768e-04 1.0966036e-03 1.6786222e-01 8.3083922e-01]]\n",
      "[[2.1541487e-06 7.8220285e-07 2.1297728e-06 9.9999499e-01]]\n",
      "[[1.4918790e-05 1.1484728e-05 8.2987797e-05 9.9989057e-01]]\n",
      "[[9.9935776e-01 1.2402447e-09 3.1079790e-07 6.4194395e-04]]\n",
      "[[9.9871862e-01 1.5333198e-06 5.3564180e-04 7.4422621e-04]]\n",
      "[[3.2106765e-08 3.1373168e-07 9.9999321e-01 6.4372425e-06]]\n",
      "[[0.42401055 0.00159795 0.02899794 0.5453936 ]]\n",
      "===\n",
      "[1, 3, 2, 3]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 480\n",
      "Average execution time per frame: 0.27807007332642875\n",
      "Predicted sentence: ['hello', 'i-love-you', 'thanks', 'i-love-you']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_7.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.7601414e-06 2.6713902e-04 8.9478322e-05 9.9963760e-01]]\n",
      "===\n",
      "[3]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 72\n",
      "Average execution time per frame: 0.24871366884973314\n",
      "Predicted sentence: ['i-love-you']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_6.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [1] ily\n",
    "# [2] ily\n",
    "# [3]\n",
    "# [4]\n",
    "# [5] ily\n",
    "# [6] ily\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
