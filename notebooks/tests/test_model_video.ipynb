{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(model_asset_path=\"../tasks/hand_landmarker.task\")\n",
    "pose_base_options = python.BaseOptions(model_asset_path=\"../tasks/pose_landmarker.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.6,\n",
    "    min_hand_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.1,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.6,\n",
    "    min_pose_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.1,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Mediapipe Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_hand_landmark = np.zeros((2, 21, 3))  # right hand and left hand\n",
    "empty_pose_landmark = np.zeros(33 * 3)\n",
    "\n",
    "def to_landmark_data(\n",
    "    hand_results: vision.HandLandmarkerResult, pose_results: vision.PoseLandmarkerResult\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract keypoints from pose and hand results for dataset creation.\n",
    "    \"\"\"\n",
    "    pose_landmark = empty_pose_landmark\n",
    "    hand_landmark = empty_hand_landmark\n",
    "\n",
    "    if pose_results.pose_world_landmarks:\n",
    "        pose_landmark = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in pose_results.pose_world_landmarks[0]]\n",
    "        ).flatten()\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hlm in enumerate(hand_results.hand_world_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmark[handedness] = np.array([[lm.x, lm.y, lm.z] for lm in hlm])\n",
    "\n",
    "    return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 4)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return pose_landmarks, hand_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action lables\n",
    "ACTIONS = np.array([\n",
    "    \"_\", \"hello\", \"what's up\", \"how\",\n",
    "    \"thanks\", \"you\", \"morning\", \"afternoon\",\n",
    "    \"night\", \"me\", \"name\", \"fine\",\n",
    "    \"happy\", \"yes\", \"no\", \"repeat\",\n",
    "    \"please\", \"want\", \"good bye\", \"learn\",\n",
    "])\n",
    "\n",
    "val_path = \"../../storage/datasets/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_version=None):\n",
    "    model_dir = \"../../storage/models/keras\"\n",
    "    prefix = \"singa_slr_v_\"\n",
    "\n",
    "    if model_version:\n",
    "        version = f\"{prefix}{model_version}.keras\"\n",
    "        ks_file = os.path.join(model_dir, version)\n",
    "\n",
    "        model = tf.keras.models.load_model(ks_file)\n",
    "\n",
    "        return version, model\n",
    "\n",
    "    model_files = os.listdir(model_dir)\n",
    "\n",
    "    # filter model files by filename prefix\n",
    "    versions = [file for file in model_files if file.startswith(prefix)]\n",
    "\n",
    "    # extract version numbers from filenames\n",
    "    versions = [file.split(\"_\")[-1] for file in versions]\n",
    "\n",
    "    # convert version numbers to tuples of integers for comparison\n",
    "    versions_int = [tuple(map(int, v.split(\".\")[0])) for v in versions]\n",
    "\n",
    "    # find the index of the latest version\n",
    "    latest_index = versions_int.index(max(versions_int))\n",
    "\n",
    "    # load the latest model\n",
    "    latest_model_path = model_files[latest_index]\n",
    "\n",
    "    model = tf.keras.models.load_model(os.path.join(model_dir, latest_model_path))\n",
    "\n",
    "    return latest_model_path, model\n",
    "\n",
    "\n",
    "v, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using model singa_slr_v_013.keras'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"using model {v}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">340</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m225\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m225\u001b[0m)        │           \u001b[38;5;34m900\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │         \u001b[38;5;34m3,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m49,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m340\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">470,734</span> (1.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m470,734\u001b[0m (1.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">156,638</span> (611.87 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m156,638\u001b[0m (611.87 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">818</span> (3.20 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m818\u001b[0m (3.20 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">313,278</span> (1.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m313,278\u001b[0m (1.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import concurrent.futures\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, image):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # convert into mediapipe numpy type support uint8, uint16, or float32\n",
    "    image = np.fliplr(image)\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # convert cv image to mediapipe image format before being passed to detectors\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "    hand_results = hand_detector.detect(image=mp_image)\n",
    "    pose_results = pose_detector.detect(image=mp_image)\n",
    "\n",
    "    landmarks = to_landmark_data(hand_results, pose_results)\n",
    "\n",
    "    return frame, landmarks, time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_video(vid):\n",
    "    clip = VideoFileClip(vid)\n",
    "\n",
    "    avg_exec_time = []\n",
    "\n",
    "    predictions = []\n",
    "    sequences = []\n",
    "\n",
    "    sentence = []\n",
    "    threshold = 0.95\n",
    "    skip_word = \"_\"\n",
    "\n",
    "    results = []\n",
    "    batch_size = 60\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_frame = {\n",
    "            executor.submit(\n",
    "                process_frame,\n",
    "                frame,\n",
    "                image,\n",
    "            ): frame\n",
    "            for frame, image in enumerate(clip.iter_frames(fps=clip.fps))\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_frame):\n",
    "            frame, landmarks, exec_time = future.result()\n",
    "            avg_exec_time.append(exec_time)\n",
    "\n",
    "            if landmarks is not None:\n",
    "                results.append((frame, landmarks))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # sort the results by frame number to ensure the order is correct\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    print(\"res:\", len(results))\n",
    "\n",
    "    for _, landmarks in results:\n",
    "        sequences.append(landmarks)\n",
    "\n",
    "        if len(sequences) < batch_size:\n",
    "            continue\n",
    "\n",
    "        # ensure correct input shape by adding an extra dimension for batch size\n",
    "        batch_motion = np.expand_dims(np.stack(sequences[-batch_size:]), axis=0)\n",
    "\n",
    "        # predict the motion\n",
    "        result = model.predict(batch_motion, verbose=0)[0]\n",
    "\n",
    "        # get the predicted class and its confidence\n",
    "        predicted = np.argmax(result)\n",
    "        confidence = result[predicted]\n",
    "\n",
    "        # append to the predictions and accuracies list\n",
    "        predictions.append(predicted)\n",
    "\n",
    "        # only keep the last 20 predictions and their accuracies\n",
    "        predictions = predictions[-20:]\n",
    "\n",
    "        predicted_sentence = ACTIONS[predicted]\n",
    "\n",
    "        # determine most frequent prediction\n",
    "        most_frequent_prediction = np.bincount(predictions[-10:]).argmax()\n",
    "\n",
    "        if most_frequent_prediction != predicted:\n",
    "            continue\n",
    "\n",
    "        elif confidence < threshold:\n",
    "            continue\n",
    "\n",
    "        elif predicted_sentence == skip_word:\n",
    "            continue\n",
    "\n",
    "        elif not sentence or predicted_sentence != sentence[-1]:\n",
    "            sentence.append(predicted_sentence)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    return (\n",
    "        sentence,\n",
    "        len(results),\n",
    "        {\n",
    "            \"pred_exec_time\": end_time,\n",
    "            \"avg_exec_time\": avg_exec_time,\n",
    "            \"total_exec_time\": sum(avg_exec_time),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: 601\n",
      "==================================================\n",
      "Total frame calculated: 601\n",
      "Total prediction execution: 25.114965200424194\n",
      "Average execution time per frame: 0.36824023188053073\n",
      "Predicted sentence: ['hello', 'morning', 'how', 'you', 'me', 'fine']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./demo/1.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Total prediction execution:\", np.mean(exec_time[\"pred_exec_time\"]))\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
