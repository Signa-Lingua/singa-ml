{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(model_asset_path=\"../tasks/hand_landmarker.task\")\n",
    "pose_base_options = python.BaseOptions(model_asset_path=\"../tasks/pose_landmarker.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.8,\n",
    "    min_hand_presence_confidence=0.9,\n",
    "    min_tracking_confidence=0.8,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.95,\n",
    "    min_pose_presence_confidence=0.95,\n",
    "    min_tracking_confidence=0.95,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Mediapipe Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_hand_landmark = np.zeros((2, 21, 3))  # right hand and left hand\n",
    "empty_pose_landmark = np.zeros(33 * 3)\n",
    "\n",
    "def to_landmark_data(\n",
    "    hand_results: vision.HandLandmarkerResult, pose_results: vision.PoseLandmarkerResult\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract keypoints from pose and hand results for dataset creation.\n",
    "    \"\"\"\n",
    "    pose_landmark = empty_pose_landmark\n",
    "    hand_landmark = empty_hand_landmark\n",
    "\n",
    "    if pose_results.pose_world_landmarks:\n",
    "        pose_landmark = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in pose_results.pose_world_landmarks[0]]\n",
    "        ).flatten()\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hlm in enumerate(hand_results.hand_world_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmark[handedness] = np.array([[lm.x, lm.y, lm.z] for lm in hlm])\n",
    "\n",
    "    return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 4)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return pose_landmarks, hand_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action lables\n",
    "ACTIONS = [\n",
    "    \"_\", \"hello\", \"thanks\", \"i-love-you\", \"see-you-later\", \"I\", \"Father\", \"Mother\", \"Yes\",\n",
    "    \"No\", \"Help\", \"Please\", \"Want\", \"What\", \"Again\", \"Eat\", \"Milk\", \"More\", \"Go To\",\n",
    "    \"Bathroom\", \"Fine\", \"Like\", \"Learn\", \"Sign\", \"Done\"\n",
    "]\n",
    "\n",
    "# limit to x actions for preprocessing\n",
    "# NOTE: change this number into the amount of the dataset labels (if changed)\n",
    "ACTIONS = np.array(ACTIONS[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(use_latest=True, version=\"\"):\n",
    "    model_dir = \"../../storage/models/keras\"\n",
    "    prefix = \"singa_slr_v_\"\n",
    "\n",
    "    if not use_latest and not version:\n",
    "        _version = os.path.join(model_dir, f\"{prefix}{version}\")\n",
    "\n",
    "        tf.keras.models.load_model(_version)\n",
    "\n",
    "    model_files = os.listdir(model_dir)\n",
    "\n",
    "    # filter model files by filename prefix\n",
    "    versions = [file for file in model_files if file.startswith(prefix)]\n",
    "\n",
    "    # extract version numbers from filenames\n",
    "    versions = [file.split(\"_\")[-1] for file in versions]\n",
    "\n",
    "    # convert version numbers to tuples of integers for comparison\n",
    "    versions_int = [tuple(map(int, v.split(\".\")[0])) for v in versions]\n",
    "\n",
    "    # find the index of the latest version\n",
    "    latest_index = versions_int.index(max(versions_int))\n",
    "\n",
    "    # load the latest model\n",
    "    latest_model_path = model_files[latest_index]\n",
    "\n",
    "    return tf.keras.models.load_model(os.path.join(model_dir, latest_model_path))\n",
    "\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    (245, 117, 16),\n",
    "    (117, 245, 16),\n",
    "    (16, 117, 245),\n",
    "    (117, 117, 16),\n",
    "    (16, 245, 117),\n",
    "    (245, 117, 245),\n",
    "]\n",
    "\n",
    "\n",
    "def confidence_bar(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(\n",
    "            output_frame,\n",
    "            (0, 60 + num * 40),\n",
    "            (int(prob * 100), 90 + num * 40),\n",
    "            colors[num],\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            output_frame,\n",
    "            actions[num],\n",
    "            (0, 85 + num * 40),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, image, threshold, skip_word):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert into mediapipe numpy type support uint8, uint16, or float32\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # Convert cv image to mediapipe image format before being passed to detectors\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "    try:\n",
    "        hand_results = hand_detector.detect(image=mp_image)\n",
    "        pose_results = pose_detector.detect(image=mp_image)\n",
    "\n",
    "        landmarks = to_landmark_data(hand_results, pose_results)\n",
    "    except:\n",
    "        print(f\"frame {frame} skipped\")\n",
    "        return frame, None, time.time() - start_time\n",
    "\n",
    "    return frame, landmarks, time.time() - start_time\n",
    "\n",
    "\n",
    "def predict_from_video(vid):\n",
    "    clip = VideoFileClip(vid)\n",
    "\n",
    "    avg_exec_time = []\n",
    "\n",
    "    predictions = []\n",
    "    sequences = []\n",
    "\n",
    "    sentence = []\n",
    "    threshold = 0.2\n",
    "    skip_word = \"_\"\n",
    "\n",
    "    results = []\n",
    "    batch_size = 60\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_frame = {\n",
    "            executor.submit(\n",
    "                process_frame,\n",
    "                frame,\n",
    "                image,\n",
    "                threshold,\n",
    "                skip_word,\n",
    "            ): frame\n",
    "            for frame, image in enumerate(clip.iter_frames(fps=clip.fps))\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_frame):\n",
    "            frame, landmarks, exec_time = future.result()\n",
    "            avg_exec_time.append(exec_time)\n",
    "\n",
    "            if landmarks is not None:\n",
    "                results.append((frame, landmarks))\n",
    "\n",
    "    # sort the results by frame number to ensure the order is correct\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    for _, landmarks in results:\n",
    "        sequences.append(landmarks)\n",
    "\n",
    "        if len(sequences) < batch_size:\n",
    "            continue\n",
    "\n",
    "        # collect a batch of sequences\n",
    "        batch_motion = np.stack(sequences[-batch_size:])\n",
    "        sequences = sequences[\n",
    "            -(batch_size - 40) :\n",
    "        ]  # keep the last 20 sequences for overlap\n",
    "\n",
    "        # ensure correct input shape by adding an extra dimension for batch size\n",
    "        batch_motion = np.expand_dims(batch_motion, axis=0)\n",
    "\n",
    "        # predict the entire batch\n",
    "        batch_result = model.predict(batch_motion, verbose=0)\n",
    "\n",
    "        print(batch_result)\n",
    "\n",
    "        for result in batch_result:\n",
    "            # len of results is 480 (which is the total frame)?\n",
    "            predicted = np.argmax(result)\n",
    "\n",
    "            if (not result[predicted] > threshold) or not (\n",
    "                ACTIONS[predicted] != skip_word\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            if not predictions or predicted != predictions[-1]:\n",
    "                predictions.append(predicted)\n",
    "\n",
    "    print(\"===\")\n",
    "    print(predictions)\n",
    "    print(\"===\")\n",
    "\n",
    "    for motion in predictions:\n",
    "        sentence.append(ACTIONS[motion])\n",
    "\n",
    "    return (\n",
    "        sentence,\n",
    "        len(results),\n",
    "        {\n",
    "            \"avg_exec_time\": avg_exec_time,\n",
    "            \"total_exec_time\": sum(avg_exec_time),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1759320e-05 9.9997890e-01 1.4928897e-06 7.8213852e-06]]\n",
      "[[2.5426933e-05 9.9995887e-01 1.0596132e-06 1.4669377e-05]]\n",
      "[[0.00085969 0.82801086 0.16679908 0.00433033]]\n",
      "[[9.9994123e-01 1.3760280e-07 3.5500059e-05 2.3141809e-05]]\n",
      "[[0.02984007 0.00289671 0.06019823 0.90706503]]\n",
      "[[1.1856037e-04 8.1997598e-05 5.0346971e-06 9.9979442e-01]]\n",
      "[[3.6143523e-04 1.3123474e-03 6.8629533e-03 9.9146324e-01]]\n",
      "[[9.9944884e-01 1.7254035e-07 2.1880467e-06 5.4880994e-04]]\n",
      "[[9.9941373e-01 4.1730927e-06 9.7303775e-05 4.8483367e-04]]\n",
      "[[2.9218388e-06 7.7728146e-06 9.9992096e-01 6.8270856e-05]]\n",
      "[[0.84600013 0.00176073 0.00352284 0.14871635]]\n",
      "===\n",
      "[1, 3, 2]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 480\n",
      "Average execution time per frame: 0.29622118721405666\n",
      "Predicted sentence: ['hello', 'i-love-you', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_7.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00178309 0.06660198 0.01177465 0.9198402 ]]\n",
      "===\n",
      "[3]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 72\n",
      "Average execution time per frame: 0.260866211520301\n",
      "Predicted sentence: ['i-love-you']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_6.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [1] ily\n",
    "# [2] ily\n",
    "# [3]\n",
    "# [4]\n",
    "# [5] ily\n",
    "# [6] ily\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
