{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(model_asset_path=\"../tasks/hand_landmarker.task\")\n",
    "pose_base_options = python.BaseOptions(model_asset_path=\"../tasks/pose_landmarker.task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.8,\n",
    "    min_hand_presence_confidence=0.9,\n",
    "    min_tracking_confidence=0.8,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.95,\n",
    "    min_pose_presence_confidence=0.95,\n",
    "    min_tracking_confidence=0.95,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Mediapipe Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_hand_landmark = np.zeros((2, 21, 3))  # right hand and left hand\n",
    "empty_pose_landmark = np.zeros(33 * 3)\n",
    "\n",
    "def to_landmark_data(\n",
    "    hand_results: vision.HandLandmarkerResult, pose_results: vision.PoseLandmarkerResult\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract keypoints from pose and hand results for dataset creation.\n",
    "    \"\"\"\n",
    "    pose_landmark = empty_pose_landmark\n",
    "    hand_landmark = empty_hand_landmark\n",
    "\n",
    "    if pose_results.pose_world_landmarks:\n",
    "        pose_landmark = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in pose_results.pose_world_landmarks[0]]\n",
    "        ).flatten()\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hlm in enumerate(hand_results.hand_world_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmark[handedness] = np.array([[lm.x, lm.y, lm.z] for lm in hlm])\n",
    "\n",
    "    return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 4)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return pose_landmarks, hand_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action lables\n",
    "ACTIONS = [\n",
    "    \"_\", \"hello\", \"thanks\", \"i-love-you\", \"I\", \"Yes\", \"No\", \"Help\", \"Please\",\n",
    "    \"Want\", \"Eat\", \"More\", \"Bathroom\", \"Learn\", \"Sign\",\n",
    "]\n",
    "\n",
    "# limit to x actions for preprocessing\n",
    "# NOTE: change this number into the amount of the dataset labels (if changed)\n",
    "ACTIONS = np.array(ACTIONS[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_version=None):\n",
    "    model_dir = \"../../storage/models/keras\"\n",
    "    prefix = \"singa_slr_v_\"\n",
    "\n",
    "    if model_version:\n",
    "        version = f\"{prefix}{model_version}.keras\"\n",
    "        ks_file = os.path.join(model_dir, version)\n",
    "\n",
    "        model = tf.keras.models.load_model(ks_file)\n",
    "\n",
    "        return version, model\n",
    "\n",
    "    model_files = os.listdir(model_dir)\n",
    "\n",
    "    # filter model files by filename prefix\n",
    "    versions = [file for file in model_files if file.startswith(prefix)]\n",
    "\n",
    "    # extract version numbers from filenames\n",
    "    versions = [file.split(\"_\")[-1] for file in versions]\n",
    "\n",
    "    # convert version numbers to tuples of integers for comparison\n",
    "    versions_int = [tuple(map(int, v.split(\".\")[0])) for v in versions]\n",
    "\n",
    "    # find the index of the latest version\n",
    "    latest_index = versions_int.index(max(versions_int))\n",
    "\n",
    "    # load the latest model\n",
    "    latest_model_path = model_files[latest_index]\n",
    "\n",
    "    model = tf.keras.models.load_model(os.path.join(model_dir, latest_model_path))\n",
    "\n",
    "    return latest_model_path, model\n",
    "\n",
    "\n",
    "v, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'using model singa_slr_v_002.keras'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"using model {v}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    (245, 117, 16),\n",
    "    (117, 245, 16),\n",
    "    (16, 117, 245),\n",
    "    (117, 117, 16),\n",
    "    (16, 245, 117),\n",
    "    (245, 117, 245),\n",
    "]\n",
    "\n",
    "\n",
    "def confidence_bar(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(\n",
    "            output_frame,\n",
    "            (0, 60 + num * 40),\n",
    "            (int(prob * 100), 90 + num * 40),\n",
    "            colors[num],\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        cv2.putText(\n",
    "            output_frame,\n",
    "            actions[num],\n",
    "            (0, 85 + num * 40),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame, image, threshold, skip_word):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert into mediapipe numpy type support uint8, uint16, or float32\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # Convert cv image to mediapipe image format before being passed to detectors\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "    try:\n",
    "        hand_results = hand_detector.detect(image=mp_image)\n",
    "        pose_results = pose_detector.detect(image=mp_image)\n",
    "\n",
    "        landmarks = to_landmark_data(hand_results, pose_results)\n",
    "    except:\n",
    "        print(f\"frame {frame} skipped\")\n",
    "        return frame, None, time.time() - start_time\n",
    "\n",
    "    return frame, landmarks, time.time() - start_time\n",
    "\n",
    "\n",
    "def predict_from_video(vid):\n",
    "    clip = VideoFileClip(vid)\n",
    "\n",
    "    avg_exec_time = []\n",
    "\n",
    "    predictions = []\n",
    "    sequences = []\n",
    "\n",
    "    sentence = []\n",
    "    threshold = 0.9\n",
    "    skip_word = \"_\"\n",
    "\n",
    "    results = []\n",
    "    batch_size = 60\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_frame = {\n",
    "            executor.submit(\n",
    "                process_frame,\n",
    "                frame,\n",
    "                image,\n",
    "                threshold,\n",
    "                skip_word,\n",
    "            ): frame\n",
    "            for frame, image in enumerate(clip.iter_frames(fps=clip.fps))\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_frame):\n",
    "            frame, landmarks, exec_time = future.result()\n",
    "            avg_exec_time.append(exec_time)\n",
    "\n",
    "            if landmarks is not None:\n",
    "                results.append((frame, landmarks))\n",
    "\n",
    "    # sort the results by frame number to ensure the order is correct\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    for _, landmarks in results:\n",
    "        sequences.append(landmarks)\n",
    "\n",
    "        if len(sequences) < batch_size:\n",
    "            continue\n",
    "\n",
    "        # collect a batch of sequences\n",
    "        batch_motion = np.stack(sequences[-batch_size:])\n",
    "        # sequences = sequences[\n",
    "        #     -(batch_size - 50) :\n",
    "        # ]  # keep the last 10 sequences for overlap\n",
    "        sequences = []\n",
    "\n",
    "        # ensure correct input shape by adding an extra dimension for batch size\n",
    "        batch_motion = np.expand_dims(batch_motion, axis=0)\n",
    "\n",
    "        # predict the entire batch\n",
    "        batch_result = model.predict(batch_motion, verbose=0)\n",
    "\n",
    "        print(batch_result)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        for result in batch_result:\n",
    "            # len of results is 480 (which is the total frame)?\n",
    "            predicted = np.argmax(result)\n",
    "\n",
    "            if (not result[predicted] > threshold) or not (\n",
    "                ACTIONS[predicted] != skip_word\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            if not predictions or predicted != predictions[-1]:\n",
    "                predictions.append(predicted)\n",
    "\n",
    "    print(\"===\")\n",
    "    print(predictions)\n",
    "    print(\"===\")\n",
    "\n",
    "    for motion in predictions:\n",
    "        sentence.append(ACTIONS[motion])\n",
    "\n",
    "    return (\n",
    "        sentence,\n",
    "        len(results),\n",
    "        {\n",
    "            \"avg_exec_time\": avg_exec_time,\n",
    "            \"total_exec_time\": sum(avg_exec_time),\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.9038884e-07 9.9983108e-01 6.9201727e-05 2.1489195e-05 1.0367809e-05\n",
      "  2.1885424e-05 4.3587061e-06 4.1192088e-05]]\n",
      "==================================================\n",
      "[[2.9635783e-07 9.9982822e-01 7.2698116e-05 2.1974707e-05 1.0653935e-05\n",
      "  2.1973745e-05 4.3730915e-06 3.9761049e-05]]\n",
      "==================================================\n",
      "[[9.9717218e-01 2.0220684e-06 4.2375018e-06 2.9208300e-07 7.8711273e-06\n",
      "  1.9463607e-05 7.2564317e-06 2.7867183e-03]]\n",
      "==================================================\n",
      "[[3.4008247e-07 4.1912940e-06 2.7568228e-07 9.9977738e-01 5.0843382e-06\n",
      "  1.9079327e-04 2.1303342e-05 5.3800824e-07]]\n",
      "==================================================\n",
      "[[2.5288571e-06 5.6290960e-06 2.9149618e-05 9.9919325e-01 3.0110471e-04\n",
      "  2.9101246e-04 1.7730684e-04 1.1518802e-07]]\n",
      "==================================================\n",
      "[[9.96232450e-01 2.72789894e-05 1.13081434e-04 4.07205807e-05\n",
      "  2.08293786e-05 1.01911246e-04 4.32418077e-04 3.03132879e-03]]\n",
      "==================================================\n",
      "[[1.3429212e-06 8.9637487e-04 9.8865885e-01 1.0421810e-02 1.1512624e-05\n",
      "  2.8182117e-07 8.9977157e-06 8.7589046e-07]]\n",
      "==================================================\n",
      "[[2.6108744e-04 6.4633363e-01 2.7772731e-01 6.1119296e-02 6.3538747e-03\n",
      "  1.4665833e-04 4.4334140e-03 3.6247284e-03]]\n",
      "==================================================\n",
      "===\n",
      "[1, 3, 2]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 480\n",
      "Average execution time per frame: 0.27878471116224923\n",
      "Predicted sentence: ['hello', 'i-love-you', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_7.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.1363489e-04 7.9979444e-01 8.9973181e-02 9.1978617e-02 4.6807304e-03\n",
      "  4.0854994e-04 1.0393575e-02 2.0572590e-03]]\n",
      "[[5.4419711e-06 2.2972850e-04 2.4977826e-06 9.9724305e-01 1.5904606e-04\n",
      "  1.6380600e-03 7.2130421e-04 9.7168765e-07]]\n",
      "[[1.5605018e-06 1.9727365e-04 2.4612361e-06 9.9911767e-01 8.0260463e-05\n",
      "  3.2108623e-04 2.7942861e-04 2.8263034e-07]]\n",
      "[[2.53623284e-06 1.04762170e-04 1.71410193e-06 9.98634875e-01\n",
      "  1.10564935e-04 7.43687095e-04 4.01124387e-04 6.58157091e-07]]\n",
      "[[4.22600315e-05 1.47881598e-04 1.90514572e-06 9.66239154e-01\n",
      "  6.34514785e-04 2.97440998e-02 3.17958998e-03 1.07539945e-05]]\n",
      "[[8.1441449e-06 3.6908383e-04 1.6584617e-06 9.9458718e-01 5.9095943e-05\n",
      "  2.2759386e-03 2.6971055e-03 1.8162965e-06]]\n",
      "[[1.7057497e-06 1.9037945e-04 2.5090528e-06 9.9905318e-01 9.4927142e-05\n",
      "  3.7264358e-04 2.8433540e-04 3.0863777e-07]]\n",
      "[[1.4019095e-06 1.9512053e-04 2.4070468e-06 9.9917728e-01 7.5313459e-05\n",
      "  2.8599522e-04 2.6221081e-04 2.5986239e-07]]\n",
      "===\n",
      "[3]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 523\n",
      "Average execution time per frame: 0.20234739939979568\n",
      "Predicted sentence: ['i-love-you']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_2.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [1] ily\n",
    "# [2] ily\n",
    "# [3]\n",
    "# [4]\n",
    "# [5] ily\n",
    "# [6] ily\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9590009e-01 4.8043617e-05 2.5281514e-04 1.1807740e-04 2.4343699e-05\n",
      "  1.5194310e-04 8.9926820e-04 2.6054278e-03]]\n",
      "[[7.2242062e-07 9.9925929e-01 9.0806236e-05 4.9496768e-04 4.6027153e-06\n",
      "  7.8607460e-05 4.6941521e-05 2.4083156e-05]]\n",
      "[[4.3365907e-07 7.5851800e-05 3.2848751e-04 9.9911684e-01 2.7582049e-04\n",
      "  6.1945524e-05 1.4061680e-04 2.1855124e-08]]\n",
      "[[1.4778279e-06 8.5616054e-04 9.9909544e-01 1.4573611e-05 3.1955027e-05\n",
      "  4.7149403e-08 2.5412004e-07 7.1912766e-08]]\n",
      "[[9.9493849e-01 5.3102231e-06 1.1562866e-05 5.2806239e-08 1.7457731e-06\n",
      "  8.5183874e-06 3.0405242e-06 5.0312979e-03]]\n",
      "[[9.9971336e-01 1.7990060e-07 6.4763321e-07 3.2386303e-09 4.9042317e-08\n",
      "  7.8637987e-07 1.4240439e-06 2.8365120e-04]]\n",
      "[[6.45591062e-04 8.28661806e-10 2.00400918e-09 7.57516361e-09\n",
      "  1.10255644e-01 8.11898190e-06 1.10784754e-06 8.89089465e-01]]\n",
      "[[1.6217276e-02 4.7787180e-05 4.6408135e-07 1.8161900e-03 1.2249707e-03\n",
      "  9.2658293e-01 5.3586498e-02 5.2385469e-04]]\n",
      "[[8.0922997e-01 6.4996261e-06 4.5744684e-07 3.7238526e-06 4.6691563e-04\n",
      "  1.6841167e-01 5.5844979e-03 1.6296268e-02]]\n",
      "[[9.9083591e-01 3.3227570e-07 8.9590816e-07 2.4079293e-08 8.7731078e-06\n",
      "  3.8055168e-04 6.0265393e-05 8.7132119e-03]]\n",
      "[[1.0597993e-05 1.2753721e-06 1.4653430e-13 3.8191914e-08 8.5621316e-04\n",
      "  9.9912733e-01 4.0355626e-06 3.8291708e-07]]\n",
      "[[9.0129251e-05 1.1867890e-08 1.1880165e-12 1.9500026e-06 8.6318847e-05\n",
      "  9.9790251e-01 1.9132758e-03 5.7675315e-06]]\n",
      "[[7.33624622e-02 1.13831405e-07 1.92136383e-07 2.65315681e-09\n",
      "  6.84708866e-05 7.44324370e-06 8.41442784e-07 9.26560521e-01]]\n",
      "[[1.9932913e-06 5.1837246e-09 1.1653585e-06 4.5429940e-10 5.0010149e-05\n",
      "  2.0369201e-07 4.3317547e-07 9.9994612e-01]]\n",
      "[[6.1354462e-05 8.5532221e-08 1.8761031e-09 5.7654233e-07 8.9007640e-01\n",
      "  8.0204372e-05 7.5237176e-07 1.0978060e-01]]\n",
      "[[6.4366188e-09 2.7719020e-08 3.0592373e-06 1.4433696e-06 9.9998939e-01\n",
      "  3.4864982e-07 2.0903590e-08 5.6347149e-06]]\n",
      "[[9.9185014e-01 9.8313331e-09 4.2298574e-08 3.5251399e-10 6.6884394e-07\n",
      "  2.4890489e-06 9.1176253e-07 8.1456909e-03]]\n",
      "[[9.7690725e-01 9.0495380e-09 2.0791154e-08 7.6435003e-10 7.5806206e-06\n",
      "  7.1567165e-06 7.3860042e-07 2.3077164e-02]]\n",
      "[[5.3751391e-01 2.2221571e-03 1.5788167e-04 3.7277532e-03 2.0976948e-02\n",
      "  3.8438436e-01 2.1358216e-02 2.9658789e-02]]\n",
      "[[6.6871995e-01 1.8799858e-03 1.7833157e-04 3.0281937e-03 1.8208986e-02\n",
      "  2.6778653e-01 1.6579373e-02 2.3618704e-02]]\n",
      "===\n",
      "[1, 3, 2, 5, 7, 4]\n",
      "===\n",
      "==================================================\n",
      "Total frame calculated: 1201\n",
      "Average execution time per frame: 0.26107713026766177\n",
      "Predicted sentence: ['hello', 'i-love-you', 'thanks', 'Yes', 'Help', 'I']\n"
     ]
    }
   ],
   "source": [
    "sentence, frame, exec_time = predict_from_video(\"./videos/test_9.mp4\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Total frame calculated:\", frame)\n",
    "print(\"Average execution time per frame:\", np.mean(exec_time[\"avg_exec_time\"]))\n",
    "print(\"Predicted sentence:\", sentence)\n",
    "\n",
    "# [1] ily\n",
    "# [2] ily\n",
    "# [3]\n",
    "# [4]\n",
    "# [5] ily\n",
    "# [6] ily\n",
    "# [7] hello, ily, ty\n",
    "# [8] hello, ily, hello, ily, ty\n",
    "# [9] hello, ily, ty, i, no, yes, help, i, ily"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
