{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E-_viZ26mYu-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From u:\\workspace\\bangkit\\capstone\\signa_lingua-vision-slr-public\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2  # type: ignore\n",
        "import os\n",
        "import time\n",
        "import numpy as np  # type: ignore\n",
        "\n",
        "import mediapipe as mp  # type: ignore\n",
        "\n",
        "from matplotlib import pyplot as plt # type: ignore\n",
        "from mediapipe.tasks import python  # type: ignore\n",
        "from mediapipe.tasks.python import vision  # type: ignore\n",
        "from mediapipe.framework.formats import landmark_pb2 # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZpDGGhmimYvA"
      },
      "outputs": [],
      "source": [
        "drawer = mp.solutions.drawing_utils # drawing utilities\n",
        "VisionRunningMode = mp.tasks.vision.RunningMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J0MMKea0mYvA"
      },
      "outputs": [],
      "source": [
        "face_base_options = python.BaseOptions(model_asset_path=\"./tasks/face_landmarker.task\")\n",
        "hand_base_options = python.BaseOptions(model_asset_path=\"./tasks/hand_landmarker.task\")\n",
        "pose_base_options = python.BaseOptions(model_asset_path=\"./tasks/pose_landmarker.task\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3CNaEbeImYvA"
      },
      "outputs": [],
      "source": [
        "face_options = vision.FaceLandmarkerOptions(\n",
        "    base_options=face_base_options,\n",
        "    output_face_blendshapes=True,\n",
        "    output_facial_transformation_matrixes=True,\n",
        "    num_faces=1,\n",
        "    running_mode=VisionRunningMode.VIDEO,\n",
        ")\n",
        "\n",
        "hand_options = vision.HandLandmarkerOptions(\n",
        "    base_options=hand_base_options,\n",
        "    num_hands=2,\n",
        "    running_mode=VisionRunningMode.VIDEO,\n",
        ")\n",
        "\n",
        "pose_options = vision.PoseLandmarkerOptions(\n",
        "    base_options=pose_base_options,\n",
        "    output_segmentation_masks=True,\n",
        "    running_mode=VisionRunningMode.VIDEO,\n",
        ")\n",
        "\n",
        "\n",
        "face_detector = vision.FaceLandmarker.create_from_options(face_options)\n",
        "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
        "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6_sXToKhmYvA"
      },
      "outputs": [],
      "source": [
        "LandmarkList = landmark_pb2.NormalizedLandmarkList\n",
        "NormalizedLandmark = landmark_pb2.NormalizedLandmark\n",
        "\n",
        "\n",
        "def create_landmark_list(landmarks, num_keypoints):\n",
        "    \"\"\"Creates a LandmarkList protocol buffer from a list of landmarks or fills with empty values if no landmarks are provided.\n",
        "\n",
        "    Args:\n",
        "        landmarks: A list of landmark objects, each containing x, y, z coordinates.\n",
        "        num_keypoints: The number of keypoints to be included in the LandmarkList.\n",
        "\n",
        "    Returns:\n",
        "        A LandmarkList containing the converted landmarks or empty values if no landmarks are provided.\n",
        "    \"\"\"\n",
        "    # generate empty landmarks with all coordinates set to 0.0\n",
        "    empty_landmarks = [\n",
        "        NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(num_keypoints)\n",
        "    ]\n",
        "\n",
        "    return LandmarkList(\n",
        "        landmark=(\n",
        "            # convert provided landmarks to NormalizedLandmark objects or use empty landmarks\n",
        "            [NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks]\n",
        "            if landmarks\n",
        "            else empty_landmarks\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def extract_keypoints_for_drawing(face_results, pose_results, hand_results):\n",
        "    \"\"\"Converts face, pose, and hand landmarks to corresponding protocol buffer lists for drawing.\n",
        "\n",
        "    Args:\n",
        "        face_results: Object containing face landmark detection results.\n",
        "        pose_results: Object containing pose landmark detection results.\n",
        "        hand_results: Object containing hand landmark detection results.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing three LandmarkList messages: face_landmarks, pose_landmarks, and hand_landmarks.\n",
        "    \"\"\"\n",
        "    # convert face landmarks to LandmarkList, using empty values if no landmarks are present\n",
        "    face_landmarks_proto = create_landmark_list(\n",
        "        face_results.face_landmarks[0] if face_results.face_landmarks else None, 478 * 3\n",
        "    )\n",
        "\n",
        "    # convert pose landmarks to LandmarkList, using empty values if no landmarks are present\n",
        "    pose_landmarks_proto = create_landmark_list(\n",
        "        pose_results.pose_landmarks[0] if pose_results.pose_landmarks else None, 33 * 4\n",
        "    )\n",
        "\n",
        "    # convert hand landmarks to LandmarkList, using empty values if no landmarks are present\n",
        "    hand_landmarks_proto = [\n",
        "        create_landmark_list(hand_landmarks, 21 * 3)\n",
        "        for hand_landmarks in (\n",
        "            hand_results.hand_landmarks\n",
        "            if hand_results.hand_landmarks\n",
        "            else [None, None]  # two hands\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return face_landmarks_proto, pose_landmarks_proto, hand_landmarks_proto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iktXTU9WmYvB"
      },
      "outputs": [],
      "source": [
        "def extract_keypoints(face_results, pose_results, hand_results):\n",
        "    \"\"\"Extracts keypoints from face, pose, and hand results for dataset creation.\n",
        "\n",
        "    Handles cases with zero, one, or two hands, assigning hand keypoints based\n",
        "    on handedness information.\n",
        "\n",
        "    Args:\n",
        "      face_results: Object containing face landmark data (if available), assumed to\n",
        "                    have a `face_landmarks` attribute with landmark data.\n",
        "      pose_results: Object containing pose landmark data (if available), assumed to\n",
        "                    have a `pose_landmarks` attribute with landmark data.\n",
        "      hand_results: Object containing hand landmark data (if available), assumed to\n",
        "                    have `hand_landmarks` and `handedness` attributes.\n",
        "\n",
        "    Returns:\n",
        "      A tuple containing three NumPy arrays representing flattened keypoints for face,\n",
        "      pose, and hand, respectively. Empty arrays are used for missing modalities.\n",
        "    \"\"\"\n",
        "\n",
        "    # extract face keypoints if available, otherwise return a zero-filled array\n",
        "    face_keypoints = (\n",
        "        np.array(\n",
        "            [\n",
        "                [landmark.x, landmark.y, landmark.z]\n",
        "                for landmark in face_results.face_landmarks[0]\n",
        "            ]\n",
        "        ).flatten()\n",
        "        if face_results.face_landmarks\n",
        "        else np.zeros(478 * 3)  # 478 landmarks with 3 coordinates each (x, y, z)\n",
        "    )\n",
        "\n",
        "    # extract pose keypoints if available, otherwise return a zero-filled array\n",
        "    pose_keypoints = (\n",
        "        np.array(\n",
        "            [\n",
        "                [landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
        "                for landmark in pose_results.pose_landmarks[0]\n",
        "            ]\n",
        "        ).flatten()\n",
        "        if pose_results.pose_landmarks\n",
        "        else np.zeros(33 * 4)  # 33 landmarks with 4 values each (x, y, z, visibility)\n",
        "    )\n",
        "\n",
        "    # initialize hand keypoints with zeros for two hands (right and left),\n",
        "    # each with 21 landmarks and 3 coordinates\n",
        "    hand_keypoints = np.zeros((2, 21, 3))\n",
        "\n",
        "    # if no hand results are available, return the empty hand keypoints\n",
        "    # and concatenate it with face and pose keypoints\n",
        "    if not hand_results:\n",
        "        return np.concatenate(\n",
        "            [face_keypoints, pose_keypoints, hand_keypoints.flatten()]\n",
        "        )\n",
        "\n",
        "    # iterate over the detected hand landmarks\n",
        "    for idx in range(len(hand_results.hand_landmarks)):\n",
        "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
        "        handedness = hand_results.handedness[idx][0].index\n",
        "\n",
        "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
        "        hand_keypoints[handedness] = np.array(\n",
        "            [[lm.x, lm.y, lm.z] for lm in hand_results.hand_landmarks[idx]]\n",
        "        )\n",
        "\n",
        "    # flatten the hand keypoints array and concatenate it with face and pose keypoints\n",
        "    return np.concatenate([face_keypoints, pose_keypoints, hand_keypoints.flatten()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "shLcpsYOmYvB"
      },
      "outputs": [],
      "source": [
        "def draw_detection_landmark(\n",
        "    image,\n",
        "    face_landmarks_proto=None,\n",
        "    pose_landmarks_proto=None,\n",
        "    hand_landmarks_proto=None,\n",
        "):\n",
        "    # draw landmark face\n",
        "    drawer.draw_landmarks(\n",
        "        image,\n",
        "        face_landmarks_proto,\n",
        "        mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
        "        drawer.DrawingSpec(color=(80, 60, 20), thickness=1, circle_radius=1),\n",
        "        drawer.DrawingSpec(color=(80, 146, 241), thickness=1, circle_radius=1),\n",
        "    )\n",
        "\n",
        "    # draw landmark pose\n",
        "    drawer.draw_landmarks(\n",
        "        image,\n",
        "        pose_landmarks_proto,\n",
        "        mp.solutions.pose.POSE_CONNECTIONS,\n",
        "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
        "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
        "    )\n",
        "\n",
        "    # draw landmark for both hand (right, left)\n",
        "    for idx in range(len(hand_landmarks_proto)):\n",
        "        drawer.draw_landmarks(\n",
        "            image,\n",
        "            hand_landmarks_proto[idx],\n",
        "            mp.solutions.hands.HAND_CONNECTIONS,\n",
        "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
        "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rKxWp-44mYvB"
      },
      "outputs": [],
      "source": [
        "def calculate_fps(start_time, frames):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    return frames / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "\n",
        "def draw_fps(image, fps):\n",
        "    cv2.putText(\n",
        "        image,\n",
        "        f\"FPS: {round(fps, 2)}\",\n",
        "        (10, 40),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1.5,\n",
        "        (0, 255, 0),\n",
        "        2,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sa_lSLcQmYvC"
      },
      "outputs": [],
      "source": [
        "colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245),\n",
        "          (117, 117, 16), (16, 245, 117), (245, 117, 245)]\n",
        "\n",
        "def confidence_bar(res, actions, input_frame, colors):\n",
        "    output_frame = input_frame.copy()\n",
        "\n",
        "    for num, prob in enumerate(res):\n",
        "        cv2.rectangle(\n",
        "            output_frame,\n",
        "            (0, 60 + num * 40),\n",
        "            (int(prob * 100), 90 + num * 40),\n",
        "            colors[num],\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "        cv2.putText(\n",
        "            output_frame,\n",
        "            actions[num],\n",
        "            (0, 85 + num * 40),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            1,\n",
        "            (255, 255, 255),\n",
        "            2,\n",
        "            cv2.LINE_AA,\n",
        "        )\n",
        "\n",
        "    return output_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R0OwHQnsmYvC"
      },
      "outputs": [],
      "source": [
        "# sign action to be detected\n",
        "ACTIONS = np.array(\n",
        "    [\n",
        "        \"hello\",\n",
        "        \"thanks\",\n",
        "        \"i-love-you\",\n",
        "        \"see-you-later\",\n",
        "        \"I\",\n",
        "        \"Father\",\n",
        "        \"Mother\",\n",
        "        \"Yes\",\n",
        "        \"No\",\n",
        "        \"Help\",\n",
        "        \"Please\",\n",
        "        \"Want\",\n",
        "        \"What\",\n",
        "        \"Again\",\n",
        "        \"Eat\",\n",
        "        \"Milk\",\n",
        "        \"More\",\n",
        "        \"Go To\",\n",
        "        \"Bathroom\",\n",
        "        \"Fine\",\n",
        "        \"Like\",\n",
        "        \"Learn\",\n",
        "        \"Sign\",\n",
        "        \"Done\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "ACTIONS = ACTIONS[:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwqafXTbmYvC",
        "outputId": "2f41554a-9ad2-4640-d3ce-58b3c0634e05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['hello', 'thanks', 'i-love-you', 'see-you-later', 'I', 'Father'],\n",
              "      dtype='<U13')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ACTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oR5Lc6NlmYvC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential  # type: ignore\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, TimeDistributed, Reshape, Bidirectional  # type: ignore\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau  # type: ignore\n",
        "from tensorflow.keras.regularizers import l2  # type: ignore\n",
        "from tensorflow.keras.optimizers import Adam # type: ignore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9aF4iijZmYvD"
      },
      "outputs": [],
      "source": [
        "# the input shape (30, 1692) where 30 is the sequence length and 1692 is the number of features per frame\n",
        "input_shape = (30, 1692)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U3HIEeTmYvD",
        "outputId": "62aa9bc5-dc66-4f94-df12-f4f0152ec75c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From u:\\workspace\\bangkit\\capstone\\signa_lingua-vision-slr-public\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From u:\\workspace\\bangkit\\capstone\\signa_lingua-vision-slr-public\\venv\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "# data normalization\n",
        "model.add(BatchNormalization(input_shape=input_shape))\n",
        "\n",
        "# first Conv1D layer with L2 regularization\n",
        "model.add(\n",
        "    Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
        ")  # changed kernel size and filters\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# second Conv1D layer with L2 regularization\n",
        "model.add(\n",
        "    Conv1D(filters=128, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
        ")  # changed kernel size and filters\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# third Conv1D layer with L2 regularization\n",
        "model.add(\n",
        "    Conv1D(filters=256, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.01))\n",
        ")  # changed kernel size and filters\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# dense layer for feature extraction with L2 regularization\n",
        "model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# bidirectional LSTM layer with L2 regularization\n",
        "model.add(\n",
        "    Bidirectional(\n",
        "        LSTM(\n",
        "            512, return_sequences=False, activation=\"relu\", kernel_regularizer=l2(0.01)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# dense layers for classification with dropout for regularization\n",
        "model.add(Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))  # slightly higher dropout rate, so it's not overfitting\n",
        "model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))  # slightly higher dropout rate, so it's not overfitting\n",
        "\n",
        "model.add(Dense(ACTIONS.shape[0], activation=\"softmax\"))\n",
        "\n",
        "\n",
        "# Load pre-trained weights\n",
        "\n",
        "model.load_weights(\n",
        "    \"../models/legacy/asl-action-cnn-lstm_1l-6a-es_p30__rlr_f05_p10_lr1e6-2.9M.h5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " batch_normalization (Batch  (None, 30, 1692)          6768      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 28, 64)            324928    \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 14, 64)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 12, 128)           24704     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 6, 128)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 4, 256)            98560     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 2, 256)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2, 64)             16448     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2, 64)             0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 1024)              2363392   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               131200    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2974646 (11.35 MB)\n",
            "Trainable params: 2971262 (11.33 MB)\n",
            "Non-trainable params: 3384 (13.22 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yscolCOumYvD"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "sequence = []\n",
        "\n",
        "sentence = []\n",
        "predictions = []\n",
        "\n",
        "sequence_length = 30\n",
        "threshold = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vTi6PQoZmYvD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 460ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# set capture properties\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 600)  # set width to 600 pixels\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 600)  # set height to 600 pixels\n",
        "cap.set(cv2.CAP_PROP_FPS, 60)  # set frame rate to 60 FPS\n",
        "\n",
        "start_time = time.time()\n",
        "isQuit = False\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "\n",
        "    if not success:\n",
        "        print(\"Ignoring empty camera frame.\")\n",
        "        continue\n",
        "\n",
        "    # NOTE: using flip image will screw'ed up some of the keypoints\n",
        "    #       data for training the model later\n",
        "    # image = cv2.flip(image, 1) # flip the image horizontally for a selfie-view display.\n",
        "\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # get current frame timestamp in milliseconds\n",
        "    timestamp_ms = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
        "\n",
        "    # convert cv image to mediapipe image format before being\n",
        "    # passed to face, pose and hand detector\n",
        "    annotated_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
        "\n",
        "    face_results = face_detector.detect_for_video(\n",
        "        image=annotated_image, timestamp_ms=timestamp_ms\n",
        "    )\n",
        "\n",
        "    hand_results = hand_detector.detect_for_video(\n",
        "        image=annotated_image, timestamp_ms=timestamp_ms + 1\n",
        "    )\n",
        "\n",
        "    pose_results = pose_detector.detect_for_video(\n",
        "        image=annotated_image, timestamp_ms=timestamp_ms + 2\n",
        "    )\n",
        "\n",
        "    keypoints = extract_keypoints(face_results, pose_results, hand_results)\n",
        "    sequences.append(keypoints)\n",
        "    sequence = sequences[-30:]\n",
        "\n",
        "    face_proto, pose_proto, hand_proto = extract_keypoints_for_drawing(\n",
        "        face_results, pose_results, hand_results\n",
        "    )\n",
        "\n",
        "    draw_detection_landmark(\n",
        "        image_rgb,\n",
        "        face_landmarks_proto=face_proto,\n",
        "        pose_landmarks_proto=pose_proto,\n",
        "        hand_landmarks_proto=hand_proto,\n",
        "    )\n",
        "\n",
        "    if len(sequence) == sequence_length:\n",
        "        # predict the action label based on the sequence of keypoints\n",
        "        result = model.predict(\n",
        "            np.expand_dims(\n",
        "                sequence, axis=0\n",
        "            )  # expanded to include a batch dimension before fed to the model\n",
        "        )[0]\n",
        "\n",
        "        # action class with the highest confidence score\n",
        "        predictions.append(np.argmax(result))\n",
        "\n",
        "        # NOTE: If the current prediction matches the most common prediction over the last 10 frames,\n",
        "        #       it suggests that the current action is likely intentional and\n",
        "        #       consistent with recent actions, rather than a momentary anomaly.\n",
        "        if np.unique(predictions[-10:])[0] == np.argmax(result):\n",
        "\n",
        "            # check if the confidence score of the current prediction index is above the threshold.\n",
        "            if result[np.argmax(result)] > threshold:\n",
        "\n",
        "                # checks if there are any elements in the sentence list.\n",
        "                # If it's not empty, it means there are already recognized actions in the sentence.\n",
        "                if len(sentence) > 0:\n",
        "                    # compares the current predicted action\n",
        "                    if ACTIONS[np.argmax(result)] != sentence[-1]:\n",
        "                        sentence.append(ACTIONS[np.argmax(result)])\n",
        "                else:\n",
        "                    # no recognized actions yet\n",
        "                    sentence.append(ACTIONS[np.argmax(result)])\n",
        "\n",
        "        # limit the length of the recognized action sentence to 5 elements by\n",
        "        # keeping only the last two elements so it does not exceed the text box\n",
        "        if len(sentence) > 3:\n",
        "            sentence = sentence[-3:]\n",
        "\n",
        "        # overlay the predicted action on the image\n",
        "        image_rgb = confidence_bar(result, ACTIONS, image_rgb, colors)\n",
        "\n",
        "    cv2.rectangle(image_rgb, (0, 0), (640, 40), (245, 117, 16), -1)\n",
        "    cv2.putText(\n",
        "        image_rgb,\n",
        "        \" \".join(sentence),\n",
        "        (3, 30),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (255, 255, 255),\n",
        "        2,\n",
        "        cv2.LINE_AA,\n",
        "    )\n",
        "\n",
        "    cv2.imshow(\n",
        "        \"MediaPipe Detection\",\n",
        "        cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB),\n",
        "    )\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1692"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequences[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([ 0.44544238,  0.64115947, -0.03221599, ...,  0.68736714,\n",
              "         0.15755503, -0.03414574]),\n",
              " array([ 0.4241001 ,  0.64154488, -0.03354746, ...,  0.68954688,\n",
              "         0.18421464, -0.04341972]),\n",
              " array([ 0.41484895,  0.6365031 , -0.03500032, ...,  0.70758289,\n",
              "         0.20392089, -0.05015678]),\n",
              " array([ 0.41598654,  0.63779616, -0.03486504, ...,  0.71803403,\n",
              "         0.20868723, -0.04683249]),\n",
              " array([ 0.42308158,  0.63875502, -0.03319922, ...,  0.71029013,\n",
              "         0.22282177, -0.04957622]),\n",
              " array([ 0.42920461,  0.63961667, -0.03309668, ...,  0.70143288,\n",
              "         0.22622743, -0.0486876 ]),\n",
              " array([ 0.43300113,  0.63974082, -0.03372842, ...,  0.68814057,\n",
              "         0.22561495, -0.05403432]),\n",
              " array([ 0.43304485,  0.63810652, -0.0344034 , ...,  0.69318217,\n",
              "         0.22982906, -0.05250174]),\n",
              " array([ 0.43698582,  0.63826382, -0.03390885, ...,  0.69646907,\n",
              "         0.22186863, -0.05552857]),\n",
              " array([ 0.43681186,  0.63832694, -0.033496  , ...,  0.68825656,\n",
              "         0.21784976, -0.06004183]),\n",
              " array([ 0.4369503 ,  0.63882053, -0.03349095, ...,  0.68048686,\n",
              "         0.22215322, -0.05444964]),\n",
              " array([ 0.43645114,  0.64065099, -0.03383826, ...,  0.67816395,\n",
              "         0.2228919 , -0.05819124]),\n",
              " array([ 0.43898454,  0.64244914, -0.03347951, ...,  0.67930984,\n",
              "         0.22274382, -0.06042907]),\n",
              " array([ 0.44030315,  0.64310604, -0.03343614, ...,  0.68035072,\n",
              "         0.22209556, -0.05706677]),\n",
              " array([ 0.44419512,  0.64498448, -0.03340475, ...,  0.68344218,\n",
              "         0.21785851, -0.05862474]),\n",
              " array([ 0.44449791,  0.64649069, -0.03353748, ...,  0.68068016,\n",
              "         0.22019495, -0.05661726]),\n",
              " array([ 0.44436741,  0.64638996, -0.03373795, ...,  0.67962497,\n",
              "         0.21785271, -0.05407131]),\n",
              " array([ 0.44318867,  0.6498459 , -0.03301088, ...,  0.68234503,\n",
              "         0.21922736, -0.05742846]),\n",
              " array([ 0.43768901,  0.6565944 , -0.03394321, ...,  0.68159914,\n",
              "         0.2247431 , -0.06006991]),\n",
              " array([ 0.42218727,  0.66272932, -0.03509031, ...,  0.67128879,\n",
              "         0.22287332, -0.061813  ]),\n",
              " array([ 0.41767535,  0.66746986, -0.0350953 , ...,  0.66554195,\n",
              "         0.22828096, -0.05839055]),\n",
              " array([ 0.41836184,  0.67675442, -0.03435604, ...,  0.6639322 ,\n",
              "         0.22567043, -0.05768144]),\n",
              " array([ 0.42686543,  0.68100953, -0.03348155, ...,  0.6615271 ,\n",
              "         0.22508059, -0.0607239 ]),\n",
              " array([ 0.42916936,  0.67978984, -0.03382162, ...,  0.6614306 ,\n",
              "         0.21815895, -0.06291324]),\n",
              " array([ 0.43717051,  0.67839074, -0.03411209, ...,  0.64741504,\n",
              "         0.2005825 , -0.06212342]),\n",
              " array([ 0.44929308,  0.66203994, -0.0356414 , ...,  0.65383327,\n",
              "         0.20285341, -0.05959782]),\n",
              " array([ 0.45183387,  0.65978801, -0.03502581, ...,  0.66940683,\n",
              "         0.20588145, -0.06620695]),\n",
              " array([ 0.45594773,  0.6600616 , -0.03476332, ...,  0.67552382,\n",
              "         0.2079975 , -0.06252488]),\n",
              " array([ 0.45720953,  0.6630789 , -0.03467686, ...,  0.67477727,\n",
              "         0.20574172, -0.05966702]),\n",
              " array([ 0.45570692,  0.67044806, -0.03467299, ...,  0.67064601,\n",
              "         0.20621949, -0.06282526])]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences[-30:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[ 0.44544238,  0.64115947, -0.03221599, ...,  0.68736714,\n",
              "          0.15755503, -0.03414574],\n",
              "        [ 0.4241001 ,  0.64154488, -0.03354746, ...,  0.68954688,\n",
              "          0.18421464, -0.04341972],\n",
              "        [ 0.41484895,  0.6365031 , -0.03500032, ...,  0.70758289,\n",
              "          0.20392089, -0.05015678],\n",
              "        ...,\n",
              "        [ 0.45594773,  0.6600616 , -0.03476332, ...,  0.67552382,\n",
              "          0.2079975 , -0.06252488],\n",
              "        [ 0.45720953,  0.6630789 , -0.03467686, ...,  0.67477727,\n",
              "          0.20574172, -0.05966702],\n",
              "        [ 0.45570692,  0.67044806, -0.03467299, ...,  0.67064601,\n",
              "          0.20621949, -0.06282526]]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.expand_dims(sequence, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "enuGreNUmYvE",
        "outputId": "75af0ef2-ad69-42c0-a90c-3839b57f2a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 50ms/step\n",
            "hello\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
        "\n",
        "print(ACTIONS[np.argmax(pred)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
