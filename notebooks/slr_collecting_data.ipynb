{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for data collection\n",
    "\n",
    "Reference for how to do the sign language on [youtube](https://www.youtube.com/watch?v=0FcwzMq4iWg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for saving the dataset\n",
    "DATASET_PATH = os.path.join(\"../storage/datasets/raw\")\n",
    "\n",
    "ACTIONS = [\n",
    "    \"_\", \"hello\", \"what's up\", \"how\",\n",
    "    \"thanks\", \"you\", \"morning\", \"afternoon\",\n",
    "    \"night\", \"me\", \"name\", \"fine\",\n",
    "    \"happy\", \"yes\", \"no\", \"repeat\",\n",
    "    \"please\", \"want\", \"good bye\", \"learn\",\n",
    "]\n",
    "\n",
    "# number of videos and actions per video\n",
    "videos_per_label = 60\n",
    "frames_per_video = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CREATED] hello\n"
     ]
    }
   ],
   "source": [
    "# create dataset directories if they do not exist\n",
    "try:\n",
    "    try:\n",
    "        shutil.rmtree(DATASET_PATH)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    os.makedirs(DATASET_PATH)\n",
    "except FileExistsError:\n",
    "    print(\"Dataset folder exists, skipping creation.\")\n",
    "\n",
    "# create directories for each action\n",
    "for action in ACTIONS:\n",
    "    try:\n",
    "        os.makedirs(os.path.join(DATASET_PATH, action))\n",
    "        print(f\"[CREATED] {action}\")\n",
    "    except FileExistsError:\n",
    "        print(f\"[SKIPPED] {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the landmarker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_path(action, sequence):\n",
    "    return os.path.join(DATASET_PATH, action, f\"{sequence}.avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recording the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_starting_text(\n",
    "    img,\n",
    "):\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        \"STARTING COLLECTION\",\n",
    "        (120, 200),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1,\n",
    "        (0, 255, 0),\n",
    "        4,\n",
    "        cv2.LINE_AA,\n",
    "    )\n",
    "\n",
    "\n",
    "def display_collecting_text(img, act, seq, pos=(15, 12)):\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        f\"Collecting frames for {act} Video Number {seq}\",\n",
    "        pos,\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.5,\n",
    "        (0, 0, 255),\n",
    "        1,\n",
    "        cv2.LINE_AA,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "\n",
    "start_time = time.time()\n",
    "frames = 0\n",
    "is_quit = False\n",
    "\n",
    "# define the codec for VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "drawer = mp.solutions.drawing_utils  # Drawing utilities\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(\n",
    "    model_asset_path=\"./tasks/hand_landmarker.task\"\n",
    ")\n",
    "pose_base_options = python.BaseOptions(\n",
    "    model_asset_path=\"./tasks/pose_landmarker.task\"\n",
    ")\n",
    "\n",
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.6,\n",
    "    min_hand_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.1,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.6,\n",
    "    min_pose_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.1,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)\n",
    "\n",
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 3)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return hand_landmarks, pose_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_landmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the video capture is opened (i.e., the camera is functioning)\n",
    "while cap.isOpened():\n",
    "    # loop through each action in the predefined ACTIONS list\n",
    "    for action in ACTIONS:\n",
    "        # loop through the number of video sequences per label\n",
    "        for sequence in range(videos_per_label):\n",
    "            # initialize VideoWriter for each video sequence\n",
    "            out = cv2.VideoWriter(\n",
    "                video_path(action, sequence),  # path for saving the video\n",
    "                fourcc,                        # codec used for compression\n",
    "                60.0,                          # frames per second\n",
    "                (640, 480)                     # frame/image size (width, height)\n",
    "            )\n",
    "\n",
    "            # loop through each frame in the sequence\n",
    "            for action_length in range(frames_per_video + 1):\n",
    "                success, frame = cap.read()\n",
    "                # create a black image for pauses or displaying text\n",
    "                pause_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "\n",
    "                # if frame capture fails, ignore and continue\n",
    "                if not success:\n",
    "                    print(\"Ignoring empty camera frame\")\n",
    "                    continue\n",
    "\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                if debug_landmark:\n",
    "                    image_rgb = image_rgb.astype(np.uint8)\n",
    "                    # convert image to mediapipe image format\n",
    "                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)\n",
    "                    # detect hands and pose\n",
    "                    hand_results = hand_detector.detect(image=mp_image)\n",
    "                    pose_results = pose_detector.detect(image=mp_image)\n",
    "                    hand, pose = to_drawing_landmark(hand_results, pose_results)\n",
    "                    draw_landmark(image_rgb, hand, pose)\n",
    "\n",
    "                # for the first frame (per each sequence), display starting and collecting text\n",
    "                # this gives time for the user to adjust to the next sign language action\n",
    "                if action_length == 0:\n",
    "                    display_starting_text(pause_image)\n",
    "                    display_collecting_text(pause_image, action, sequence)\n",
    "                    cv2.imshow(\"Detecting Sign Language\", pause_image)\n",
    "                    cv2.waitKey(1500)  # wait for 1.5 seconds to give time for adjustment\n",
    "\n",
    "                # if frame reaches 60 (59 since the frame started from 0), break out of the loop\n",
    "                # NOTE: this might needed since using the last code\n",
    "                #       it only gives us only 58 frame also adding + 1\n",
    "                #       in frames_per_video loop is crucial\n",
    "                elif action_length == 60:\n",
    "                    out.write(frame)\n",
    "                    break\n",
    "\n",
    "                # for other frames, display collecting text and show the frame\n",
    "                else:\n",
    "                    display_collecting_text(image_rgb, action, sequence)\n",
    "                    cv2.imshow(\"Detecting Sign Language\", cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB))\n",
    "                    # write the frame to the video file\n",
    "                    out.write(frame)\n",
    "\n",
    "                # break the loop if 'q' key is pressed\n",
    "                if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                    is_quit = True\n",
    "                    break\n",
    "\n",
    "            # break out of the action sequence loop\n",
    "            if is_quit:\n",
    "                break\n",
    "\n",
    "        # break out of the action loop\n",
    "        if is_quit:\n",
    "            break\n",
    "\n",
    "    # release the opencv related video object\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
