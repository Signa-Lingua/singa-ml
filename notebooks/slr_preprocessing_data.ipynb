{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from moviepy.editor import VideoFileClip, ImageSequenceClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Task vision\n",
    "\n",
    "documentations for hand :\n",
    "- https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker/python#configuration_options\n",
    "- https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\n",
    "\n",
    "documentations for pose :\n",
    "- https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker#configurations_options\n",
    "- https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils  # Drawing utilities\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(\n",
    "    model_asset_path=\"./tasks/hand_landmarker.task\"\n",
    ")\n",
    "pose_base_options = python.BaseOptions(\n",
    "    model_asset_path=\"./tasks/pose_landmarker.task\"\n",
    ")\n",
    "\n",
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.6,\n",
    "    min_hand_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.1,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.6,\n",
    "    min_pose_presence_confidence=0.6,\n",
    "    min_tracking_confidence=0.1,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for dataset preprocessing\n",
    "\n",
    "Reference for how to do the sign language in [youtube](https://www.youtube.com/watch?v=0FcwzMq4iWg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset path for saving the preprocessed raw data (video)\n",
    "DATASET_PATH = os.path.join(\"../storage/datasets/cleaned\")\n",
    "DATASET_PATH_RAW = os.path.join(\"../storage/datasets/archive/raw\")\n",
    "\n",
    "# action lables\n",
    "ACTIONS = [\n",
    "    \"_\", \"hello\", \"what's up\", \"how\",\n",
    "    \"thanks\", \"you\", \"morning\", \"afternoon\",\n",
    "    \"night\", \"me\", \"name\", \"fine\",\n",
    "    \"happy\", \"yes\", \"no\", \"repeat\",\n",
    "    \"please\", \"want\", \"good bye\", \"learn\",\n",
    "]\n",
    "\n",
    "# limit to x actions for preprocessing\n",
    "# NOTE: change this number into the amount of the dataset labels (if changed)\n",
    "# ACTIONS = ACTIONS[15:16]\n",
    "\n",
    "# number of videos and actions per video\n",
    "videos_per_label = 120 # change it to how many videos you have\n",
    "frames_per_video = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repeat']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CREATED] repeat\n"
     ]
    }
   ],
   "source": [
    "# create dataset directories if they do not exist\n",
    "try:\n",
    "    try:\n",
    "        shutil.rmtree(DATASET_PATH)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    os.makedirs(DATASET_PATH)\n",
    "except FileExistsError:\n",
    "    print(\"Dataset folder exists, skipping creation\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "for action in ACTIONS:\n",
    "    os.makedirs(os.path.join(DATASET_PATH, action))\n",
    "\n",
    "    print(f\"[CREATED] {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for extracting the Mediapipe Landmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Landmarker (drawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 3)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return hand_landmarks, pose_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Landmarker (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_hand_landmark = np.zeros((2, 21, 3))  # right hand and left hand\n",
    "empty_pose_landmark = np.zeros(33 * 3)\n",
    "\n",
    "\n",
    "def to_landmark_data(\n",
    "    hand_results: vision.HandLandmarkerResult, pose_results: vision.PoseLandmarkerResult\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract keypoints from pose and hand results for dataset creation.\n",
    "    \"\"\"\n",
    "    pose_landmark = empty_pose_landmark\n",
    "    hand_landmark = empty_hand_landmark\n",
    "\n",
    "    if pose_results.pose_world_landmarks:\n",
    "        pose_landmark = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in pose_results.pose_world_landmarks[0]]\n",
    "        ).flatten()\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hlm in enumerate(hand_results.hand_world_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmark[handedness] = np.array([[lm.x, lm.y, lm.z] for lm in hlm])\n",
    "\n",
    "    return np.concatenate([pose_landmark, hand_landmark.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the landmarker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_landmark(action: str, sequence: int, keypoints: np.ndarray):\n",
    "    np_path = os.path.join(DATASET_PATH, action, str(sequence))\n",
    "\n",
    "    np.save(np_path, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the raw data and process it using mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_landmark_debug = False\n",
    "\n",
    "\n",
    "def save_video_with_landmark(image_list, fps, label, video_num):\n",
    "    images = [image for _, image, _ in image_list]\n",
    "\n",
    "    new_clip = ImageSequenceClip(images, fps=fps)\n",
    "\n",
    "    output_path = f\"../storage/datasets/debug/{label}_{video_num}.mp4\"\n",
    "\n",
    "    new_clip.write_videofile(output_path, codec=\"libx264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(label, video_num, frame, image, flip_frame, is_debug):\n",
    "    # start time for performance tracking\n",
    "    start_time = time.time()\n",
    "\n",
    "    # flip the image horizontally for a selfie-view display\n",
    "    if flip_frame: image = np.fliplr(image)\n",
    "\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # convert image to mediapipe image format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "        # detect hands and pose\n",
    "        hand_results = hand_detector.detect(image=mp_image)\n",
    "\n",
    "        pose_results = pose_detector.detect(image=mp_image)\n",
    "\n",
    "        # convert results to landmarks\n",
    "        keypoints = to_landmark_data(hand_results, pose_results)\n",
    "\n",
    "        if is_debug:\n",
    "            hand, pose = to_drawing_landmark(hand_results, pose_results)\n",
    "            draw_landmark(image, hand, pose)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {label} video {video_num} frame {frame}: {e}\")\n",
    "        return frame, None, None, time.time() - start_time\n",
    "\n",
    "    if is_debug:\n",
    "        return frame, image, keypoints, time.time() - start_time\n",
    "\n",
    "    return frame, None, keypoints, time.time() - start_time\n",
    "\n",
    "\n",
    "def process_video(label: str, video_num: int, flip_frame: bool, video_start: int = 0):\n",
    "    video_path = os.path.join(DATASET_PATH_RAW, label, f\"{video_num}.avi\")\n",
    "\n",
    "    video_counter = video_start\n",
    "\n",
    "    try:\n",
    "        clip = VideoFileClip(video_path)\n",
    "    except OSError:\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return f\"[{label}] ({video_num}) error opening video file\"\n",
    "\n",
    "    avg_exec_time = []\n",
    "    results = []\n",
    "\n",
    "    # use ThreadPoolExecutor to process frames concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_frame = {\n",
    "            executor.submit(\n",
    "                process_frame,\n",
    "                label,\n",
    "                video_num,\n",
    "                frame,\n",
    "                image,\n",
    "                flip_frame,\n",
    "                image_landmark_debug\n",
    "            ): frame\n",
    "            for frame, image in enumerate(clip.iter_frames(fps=clip.fps))\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_frame):\n",
    "            frame, image, keypoints, exec_time = future.result()\n",
    "\n",
    "            if keypoints is not None:\n",
    "                results.append((frame, image, keypoints))\n",
    "\n",
    "            avg_exec_time.append(exec_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    if image_landmark_debug:\n",
    "        save_video_with_landmark(results, clip.fps, label, video_num)\n",
    "\n",
    "    if len(results) == 60:\n",
    "        # combine all landmark sequences into a single numpy array\n",
    "        keypoints = np.array([landmark for _, _, landmark in results])\n",
    "\n",
    "        save_cleaned_landmark(label, video_counter, keypoints)\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        avg_exec_time = {\n",
    "            \"avg_video_exec\": avg_exec_time,\n",
    "            \"save_exec\": end_time,\n",
    "        }\n",
    "\n",
    "        return avg_exec_time, label, video_num, frame\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    {\"action\": word, \"frame\": i} for word in ACTIONS for i in range(videos_per_label)\n",
    "]\n",
    "\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_first_5 = [actions[i: i + 5] for i in range(0, len(actions), videos_per_label)]\n",
    "all_first_1 = [actions[i] for i in range(0, len(actions), videos_per_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_landmark_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[repeat]\t0\n",
      "[repeat]\t1\n",
      "[repeat]\t2\n",
      "[repeat]\t3\n",
      "[repeat]\t4\n",
      "[repeat]\t5\n",
      "[repeat]\t6\n",
      "[repeat]\t7\n",
      "[repeat]\t8\n",
      "[repeat]\t9\n",
      "[repeat]\t10\n",
      "[repeat]\t11\n",
      "[repeat]\t12\n",
      "[repeat]\t13\n",
      "[repeat]\t14\n",
      "[repeat]\t15\n",
      "[repeat]\t16\n",
      "[repeat]\t17\n",
      "[repeat]\t18\n",
      "[repeat]\t19\n",
      "[repeat]\t20\n",
      "[repeat]\t21\n",
      "[repeat]\t22\n",
      "[repeat]\t23\n",
      "[repeat]\t24\n",
      "[repeat]\t25\n",
      "[repeat]\t26\n",
      "[repeat]\t27\n",
      "[repeat]\t28\n",
      "[repeat]\t29\n",
      "[repeat]\t30\n",
      "[repeat]\t31\n",
      "[repeat]\t32\n",
      "[repeat]\t33\n",
      "[repeat]\t34\n",
      "[repeat]\t35\n",
      "[repeat]\t36\n",
      "[repeat]\t37\n",
      "[repeat]\t38\n",
      "[repeat]\t39\n",
      "[repeat]\t40\n",
      "[repeat]\t41\n",
      "[repeat]\t42\n",
      "[repeat]\t43\n",
      "[repeat]\t44\n",
      "[repeat]\t45\n",
      "[repeat]\t46\n",
      "[repeat]\t47\n",
      "[repeat]\t48\n",
      "[repeat]\t49\n",
      "[repeat]\t50\n",
      "[repeat]\t51\n",
      "[repeat]\t52\n",
      "[repeat]\t53\n",
      "[repeat]\t54\n",
      "[repeat]\t55\n",
      "[repeat]\t56\n",
      "[repeat]\t57\n",
      "[repeat]\t58\n",
      "[repeat]\t59\n",
      "[repeat]\t60\n",
      "[repeat]\t61\n",
      "[repeat]\t62\n",
      "[repeat]\t63\n",
      "[repeat]\t64\n",
      "[repeat]\t65\n",
      "[repeat]\t66\n",
      "[repeat]\t67\n",
      "[repeat]\t68\n",
      "[repeat]\t69\n",
      "[repeat]\t70\n",
      "[repeat]\t71\n",
      "[repeat]\t72\n",
      "[repeat]\t73\n",
      "[repeat]\t74\n",
      "[repeat]\t75\n",
      "[repeat]\t76\n",
      "[repeat]\t77\n",
      "[repeat]\t78\n",
      "[repeat]\t79\n",
      "[repeat]\t80\n",
      "[repeat]\t81\n",
      "[repeat]\t82\n",
      "[repeat]\t83\n",
      "[repeat]\t84\n",
      "[repeat]\t85\n",
      "[repeat]\t86\n",
      "[repeat]\t87\n",
      "[repeat]\t88\n",
      "[repeat]\t89\n",
      "[repeat]\t90\n",
      "[repeat]\t91\n",
      "[repeat]\t92\n",
      "[repeat]\t93\n",
      "[repeat]\t94\n",
      "[repeat]\t95\n",
      "[repeat]\t96\n",
      "[repeat]\t97\n",
      "[repeat]\t98\n",
      "[repeat]\t99\n",
      "[repeat]\t100\n",
      "[repeat]\t101\n",
      "[repeat]\t102\n",
      "[repeat]\t103\n",
      "[repeat]\t104\n",
      "[repeat]\t105\n",
      "[repeat]\t106\n",
      "[repeat]\t107\n",
      "[repeat]\t108\n",
      "[repeat]\t109\n",
      "[repeat]\t110\n",
      "[repeat]\t111\n",
      "[repeat]\t112\n",
      "[repeat]\t113\n",
      "[repeat]\t114\n",
      "[repeat]\t115\n",
      "[repeat]\t116\n",
      "[repeat]\t117\n",
      "[repeat]\t118\n",
      "[repeat]\t119\n"
     ]
    }
   ],
   "source": [
    "def process_action_frame(label, frame, log):\n",
    "    print(f\"[{label}]\\t{frame}\")\n",
    "    result1 = process_video(label, frame, False, frame)  # without flip\n",
    "\n",
    "    # the flip image should start at 120th video\n",
    "    # so in the end we have 240 data on each action\n",
    "    result2 = process_video(label, frame, True, videos_per_label + frame)  # with fip image\n",
    "\n",
    "    if not result1 or not result2:\n",
    "        return None\n",
    "\n",
    "    log.append([result1, result2])\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future_to_action = {\n",
    "        executor.submit(process_action_frame, action[\"action\"], action[\"frame\"], log): action\n",
    "        for action in actions\n",
    "    }\n",
    "\n",
    "    for future in concurrent.futures.as_completed(future_to_action):\n",
    "        log = future.result()\n",
    "\n",
    "        if not log:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
