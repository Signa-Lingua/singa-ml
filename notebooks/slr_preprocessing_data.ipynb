{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Task vision\n",
    "\n",
    "documentations for hand :\n",
    "- https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker/python#configuration_options\n",
    "- https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb\n",
    "\n",
    "documentations for pose :\n",
    "- https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker#configurations_options\n",
    "- https://github.com/google-ai-edge/mediapipe-samples/blob/main/examples/pose_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Pose_Landmarker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawer = mp.solutions.drawing_utils  # Drawing utilities\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# base options for hand and pose detection models\n",
    "hand_base_options = python.BaseOptions(\n",
    "    model_asset_path=\"./tasks/hand_landmarker.task\"\n",
    ")\n",
    "pose_base_options = python.BaseOptions(\n",
    "    model_asset_path=\"./tasks/pose_landmarker.task\"\n",
    ")\n",
    "\n",
    "# options for hand detection\n",
    "hand_options = vision.HandLandmarkerOptions(\n",
    "    base_options=hand_base_options,\n",
    "    num_hands=2,\n",
    "    min_hand_detection_confidence=0.8,\n",
    "    min_hand_presence_confidence=0.9,\n",
    "    min_tracking_confidence=0.8,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# options for pose detection\n",
    "pose_options = vision.PoseLandmarkerOptions(\n",
    "    base_options=pose_base_options,\n",
    "    output_segmentation_masks=True,\n",
    "    min_pose_detection_confidence=0.95,\n",
    "    min_pose_presence_confidence=0.95,\n",
    "    min_tracking_confidence=0.95,\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    ")\n",
    "\n",
    "# create detectors\n",
    "hand_detector = vision.HandLandmarker.create_from_options(hand_options)\n",
    "pose_detector = vision.PoseLandmarker.create_from_options(pose_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for dataset preprocessing\n",
    "\n",
    "Reference for how to do the sign language in [youtube](https://www.youtube.com/watch?v=0FcwzMq4iWg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset path for saving the preprocessed raw data (video)\n",
    "DATASET_PATH = os.path.join(\"../storage/datasets/cleaned\")\n",
    "DATASET_PATH_RAW = os.path.join(\"../storage/datasets/raw\")\n",
    "\n",
    "# action lables\n",
    "ACTIONS = [\n",
    "    \"_\", \"hello\", \"thanks\", \"i-love-you\", \"see-you-later\", \"I\", \"Father\", \"Mother\", \"Yes\",\n",
    "    \"No\", \"Help\", \"Please\", \"Want\", \"What\", \"Again\", \"Eat\", \"Milk\", \"More\", \"Go To\",\n",
    "    \"Bathroom\", \"Fine\", \"Like\", \"Learn\", \"Sign\", \"Done\"\n",
    "]\n",
    "\n",
    "# limit to x actions for preprocessing\n",
    "# NOTE: change this number into the amount of the dataset labels (if changed)\n",
    "ACTIONS = ACTIONS[:4]\n",
    "\n",
    "# number of videos and actions per video\n",
    "videos_per_label = 120 # the half of the videos only for flipped data (image)\n",
    "frames_per_video = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CREATED] _\n",
      "[CREATED] hello\n",
      "[CREATED] thanks\n",
      "[CREATED] i-love-you\n"
     ]
    }
   ],
   "source": [
    "# create dataset directories if they do not exist\n",
    "try:\n",
    "    try:\n",
    "        shutil.rmtree(DATASET_PATH)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    os.makedirs(DATASET_PATH)\n",
    "except FileExistsError:\n",
    "    print(\"Dataset folder exists, skipping creation\")\n",
    "    print(\"========================================\")\n",
    "\n",
    "for action in ACTIONS:\n",
    "    os.makedirs(os.path.join(DATASET_PATH, action))\n",
    "\n",
    "    print(f\"[CREATED] {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for extracting the Mediapipe Landmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Landmarker (drawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LandmarkList = landmark_pb2.NormalizedLandmarkList  # aliases for landmark types\n",
    "NormalizedLandmark = landmark_pb2.NormalizedLandmark  # aliases for landmark types\n",
    "\n",
    "\n",
    "def to_landmark_list(landmarks):\n",
    "    \"\"\"\n",
    "    Create a LandmarkList from a list of landmarks or fill with empty values if no landmarks are provided.\n",
    "    \"\"\"\n",
    "    return LandmarkList(\n",
    "        landmark=([NormalizedLandmark(x=lm.x, y=lm.y, z=lm.z) for lm in landmarks])\n",
    "    )\n",
    "\n",
    "\n",
    "empty_pose_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(33 * 3)]\n",
    ")\n",
    "\n",
    "empty_hand_landmarks = to_landmark_list(\n",
    "    [NormalizedLandmark(x=0.0, y=0.0, z=0.0) for _ in range(21 * 3)]\n",
    ")\n",
    "\n",
    "\n",
    "def to_drawing_landmark(hand_results, pose_results):\n",
    "    \"\"\"\n",
    "    Convert pose and hand landmarks to LandmarkList for drawing.\n",
    "    \"\"\"\n",
    "\n",
    "    pose_landmarks = (\n",
    "        to_landmark_list(pose_results.pose_landmarks[0])\n",
    "        if pose_results.pose_landmarks\n",
    "        else empty_pose_landmarks\n",
    "    )\n",
    "\n",
    "    hand_landmarks = [empty_hand_landmarks, empty_hand_landmarks]\n",
    "\n",
    "    if not hand_results:\n",
    "        return pose_landmarks, None\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hand_landmark in enumerate(hand_results.hand_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmarks[handedness] = to_landmark_list(hand_landmark)\n",
    "\n",
    "    return hand_landmarks, pose_landmarks\n",
    "\n",
    "\n",
    "def draw_landmark(image, hand_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "    Draw detected landmarks on the image.\n",
    "    \"\"\"\n",
    "    drawer.draw_landmarks(\n",
    "        image,\n",
    "        pose_landmarks,\n",
    "        mp.solutions.pose.POSE_CONNECTIONS,\n",
    "        drawer.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=3),\n",
    "        drawer.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2),\n",
    "    )\n",
    "\n",
    "    if not hand_landmarks:\n",
    "        return\n",
    "\n",
    "    for hand_landmarks in hand_landmarks:\n",
    "        drawer.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp.solutions.hands.HAND_CONNECTIONS,\n",
    "            drawer.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=2),\n",
    "            drawer.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Landmarker (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_hand_landmark = np.zeros((2, 21, 3))  # right hand and left hand\n",
    "empty_pose_landmark = np.zeros(33 * 3)\n",
    "\n",
    "\n",
    "def to_landmark_data(\n",
    "    hand_results: vision.HandLandmarkerResult, pose_results: vision.PoseLandmarkerResult\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract keypoints from pose and hand results for dataset creation.\n",
    "    \"\"\"\n",
    "    pose_landmark = empty_pose_landmark\n",
    "    hand_landmark = empty_hand_landmark\n",
    "\n",
    "    if pose_results.pose_world_landmarks:\n",
    "        pose_landmark = np.array(\n",
    "            [[lm.x, lm.y, lm.z] for lm in pose_results.pose_world_landmarks[0]]\n",
    "        ).flatten()\n",
    "\n",
    "    # if no hand results are available, return the empty hand keypoints\n",
    "    # and concatenate it with face and pose keypoints\n",
    "    if not hand_results:\n",
    "        return np.concatenate([pose_landmark, hand_landmark.flatten()])\n",
    "\n",
    "    # iterate over the detected hand landmarks\n",
    "    for index, hlm in enumerate(hand_results.hand_world_landmarks):\n",
    "        # determine the hand index (0 for right hand, 1 for left hand) using handedness information\n",
    "        handedness = hand_results.handedness[index][0].index\n",
    "\n",
    "        # extract the keypoints for the current hand and assign them to the appropriate index\n",
    "        hand_landmark[handedness] = np.array([[lm.x, lm.y, lm.z] for lm in hlm])\n",
    "\n",
    "    return np.concatenate([pose_landmark, hand_landmark.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the landmarker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_landmark(action: str, sequence: int, keypoints: np.ndarray):\n",
    "    np_path = os.path.join(DATASET_PATH, action, str(sequence))\n",
    "\n",
    "    np.save(np_path, keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the raw data and process it using mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(label, video_num, frame, image, flip_frame, timestamp_ms):\n",
    "    # start time for performance tracking\n",
    "    start_time = time.time()\n",
    "\n",
    "    # flip the image horizontally for a selfie-view display\n",
    "    if flip_frame: image = np.fliplr(image)\n",
    "\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "\n",
    "        # convert image to mediapipe image format\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "        # detect hands and pose\n",
    "        hand_results = hand_detector.detect(image=mp_image)\n",
    "\n",
    "        pose_results = pose_detector.detect(image=mp_image)\n",
    "\n",
    "        # convert results to landmarks\n",
    "        keypoints = to_landmark_data(hand_results, pose_results)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {label} video {video_num} frame {frame}: {e}\")\n",
    "        return frame, None, time.time() - start_time\n",
    "\n",
    "    return frame, keypoints, time.time() - start_time\n",
    "\n",
    "\n",
    "def process_video(label: str, video_num: int, flip_frame: bool, video_start: int = 0):\n",
    "    video_path = os.path.join(DATASET_PATH_RAW, label, f\"{video_num}.avi\")\n",
    "\n",
    "    video_counter = video_start\n",
    "\n",
    "    try:\n",
    "        clip = VideoFileClip(video_path)\n",
    "    except OSError:\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return f\"[{label}] ({video_num}) error opening video file\"\n",
    "\n",
    "    avg_exec_time = []\n",
    "    results = []\n",
    "\n",
    "    # use ThreadPoolExecutor to process frames concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_frame = {\n",
    "            executor.submit(\n",
    "                process_frame,\n",
    "                label,\n",
    "                video_num,\n",
    "                frame,\n",
    "                image,\n",
    "                flip_frame,\n",
    "                clip.duration * frame / clip.fps,\n",
    "            ): frame\n",
    "            for frame, image in enumerate(clip.iter_frames(fps=clip.fps))\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_frame):\n",
    "            frame, keypoints, exec_time = future.result()\n",
    "\n",
    "            if keypoints is not None:\n",
    "                results.append((frame, keypoints))\n",
    "\n",
    "            avg_exec_time.append(exec_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    results.sort(key=lambda x: x[0])\n",
    "\n",
    "    if len(results) == 60:\n",
    "        # combine all landmark sequences into a single numpy array\n",
    "        keypoints = np.array([landmark for _, landmark in results])\n",
    "\n",
    "        save_cleaned_landmark(label, video_counter, keypoints)\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        avg_exec_time = {\n",
    "            \"avg_video_exec\": avg_exec_time,\n",
    "            \"save_exec\": end_time,\n",
    "        }\n",
    "\n",
    "        return avg_exec_time, label, video_num, frame\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    {\"action\": word, \"frame\": i}\n",
    "    for word in ACTIONS\n",
    "    for i in range(videos_per_label // 2)\n",
    "]\n",
    "\n",
    "log = []\n",
    "all_first_5 = actions[:5] + actions[60:65] + actions[120:125] + actions[180:185]\n",
    "all_first_1 = actions[:1] + actions[60:61] + actions[120:121] + actions[180:181]\n",
    "\n",
    "for action in actions:\n",
    "    label = action[\"action\"]\n",
    "    frame = action[\"frame\"]\n",
    "\n",
    "    result1 = process_video(label, frame, False, frame)  # without flip\n",
    "    result2 = process_video(label, frame, True, 60 + frame)  # with fip image\n",
    "\n",
    "    if not result1 or not result2:\n",
    "        break\n",
    "\n",
    "    log.append([result1, result2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW : +- 14 minute ~\n",
    "# OLD : +- 24 minute ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
